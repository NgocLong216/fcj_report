[{"uri":"https://ngocLong216.github.io/fcj_report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Ngoc Long\nPhone Number: 0971513787\nEmail: nguyenngoclong216@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"InsightHR Platform Overview InsightHR is a comprehensive serverless HR automation platform that demonstrates modern cloud-native application development on AWS. This workshop will guide you through building a production-ready application from scratch.\nProject Scope The InsightHR platform provides:\nEmployee Management System: Complete CRUD operations for employee records with advanced filtering by department, position, and status Performance Tracking: Quarterly performance scores with automatic calculation based on KPIs, completed tasks, and 360-degree feedback Attendance Management: Real-time check-in/check-out system with historical tracking and status monitoring AI-Powered Chatbot: Natural language query interface using AWS Bedrock (Claude 3 Haiku) for intelligent data insights Analytics Dashboard: Interactive visualizations with charts, tables, and export capabilities Role-Based Access Control: Three-tier access system (Admin, Manager, Employee) with appropriate data filtering Secure Authentication: Email/password and Google OAuth integration via AWS Cognito Architecture Overview ┌─────────────────────────────────────────────────────────────────┐ │ User Browser │ └────────────────────────┬────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ CloudFront CDN │ │ Custom Domain: insight-hr.io.vn │ └────────────────────────┬────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ S3 Static Website │ │ React SPA (Vite + TypeScript) │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ API Gateway (REST) │ │ Cognito Authorizer │ └────────────────────────┬────────────────────────────────────────┘ │ ┌───────────────┼───────────────┐ ▼ ▼ ▼ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Lambda │ │ Lambda │ │ Lambda │ │ Auth │ │ Employees │ │ Chatbot │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ │ ▼ ▼ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ DynamoDB │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ │ │ Users │ │Employees │ │ Scores │ │Attendance│ │ │ └──────────┘ └──────────┘ └──────────┘ └──────────┘ │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ AWS Bedrock │ │ (Claude 3 Haiku Model) │ └─────────────────────────────────────────────────────────────────┘ Technology Stack Frontend:\nReact 18 with TypeScript Vite 7.2 (build tool) Tailwind CSS 3.4 (Frutiger Aero theme) Zustand 5.0 (state management) React Hook Form + Zod (form validation) Recharts 3.4 (data visualization) Backend:\nPython 3.11 (Lambda runtime) AWS Lambda (serverless compute) AWS API Gateway (REST API) AWS DynamoDB (NoSQL database) AWS Cognito (authentication) AWS Bedrock (AI/ML) Infrastructure:\nAWS S3 (static hosting) AWS CloudFront (CDN) AWS Route53 (DNS) AWS CloudWatch (monitoring) AWS IAM (security) Key Features 1. Fully Serverless Architecture No EC2 instances to manage Automatic scaling based on demand Pay-per-use pricing model High availability built-in 2. Modern Development Stack TypeScript for type safety React for responsive UI Python for backend logic Infrastructure as Code principles 3. Production-Ready Custom domain with SSL CloudWatch monitoring Synthetic canaries for testing Role-based access control 4. Cost-Effective DynamoDB on-demand pricing Lambda free tier eligible Minimal monthly costs (~$2-5) No idle resource charges Workshop Structure This workshop is divided into 11 modules:\nWorkshop Overview (Current) - Understanding the project scope Prerequisites - Setting up your environment Project Architecture - Deep dive into system design Setup AWS Environment - Configuring AWS account and credentials Database Setup - Creating and populating DynamoDB tables Authentication Service - Implementing Cognito and auth Lambda functions Backend Services - Building employee, performance, and chatbot APIs Frontend Development - Creating the React application Deployment - Deploying to S3 and CloudFront Testing \u0026amp; Monitoring - Setting up CloudWatch and canaries Cleanup - Removing resources to avoid charges Learning Objectives By the end of this workshop, you will be able to:\nDesign and implement serverless architectures on AWS Build RESTful APIs using Lambda and API Gateway Model data effectively in DynamoDB Implement authentication with AWS Cognito Integrate AI capabilities using AWS Bedrock Deploy static websites with S3 and CloudFront Monitor applications with CloudWatch Apply security best practices with IAM Optimize costs for serverless applications Prerequisites Check Before proceeding, ensure you have:\n✅ AWS Account with admin access ✅ AWS CLI installed and configured ✅ Node.js 18+ and npm installed ✅ Python 3.11+ installed ✅ Basic understanding of React and TypeScript ✅ Familiarity with REST APIs ✅ Text editor or IDE (VS Code recommended) Estimated Costs Running this workshop will incur minimal costs:\nService Estimated Cost DynamoDB $0.50/month Lambda Free tier S3 + CloudFront $1-2/month API Gateway $0.10/month Bedrock $0.0004/query Total $2-5/month Remember to complete the cleanup module at the end to avoid ongoing charges.\nNext Steps Ready to begin? Let\u0026rsquo;s move to the Prerequisites section to set up your development environment.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Clario enhances the quality of the clinical trial documentation process with Amazon Bedrock This post is co-written with Kim Nguyen and Shyam Banuprakash from Clario.\nClario is a leading provider of endpoint data solutions to the clinical trials industry, generating high-quality clinical evidence for life sciences companies seeking to bring new therapies to patients. Since Clario’s founding more than 50 years ago, the company’s endpoint data solutions have supported clinical trials more than 26,000 times with over 700 regulatory approvals across more than 100 countries. One of the critical challenges Clario faces when supporting its clients is the time-consuming process of generating documentation for clinical trials, which can take weeks.\nThe business challenge When medical imaging analysis is part of a clinical trial it is supporting, Clario prepares a medical imaging charter process document that outlines the format and requirements of the central review of clinical trial images (the Charter). Based on the Charter, Clario’s imaging team creates several subsequent documents (as shown in the following figure), including the business requirement specification (BRS), training slides, and ancillary documents. The content of these documents is largely derived from the Charter, with significant reformatting and rephrasing required. This process is time-consuming, can be subject to inadvertent manual error, and carries the risk of inconsistent or redundant information, which can delay or otherwise negatively impact the clinical trial.\nClario’s imaging team recognized the need to modernize the document generation process and streamline the processes used to create end-to-end document workflows. Clario engaged with their AWS account team and AWS Generative AI Innovation Center to explore how generative AI could help streamline the process.\nThe solution The AWS team worked closely with Clario to develop a prototype solution that uses AWS AI services to automate the BRS generation process. The solution involves the following key services:\nAmazon Simple Storage Service (Amazon S3): A scalable object storage service used to store the charter-derived and generated BRS documents.\nAmazon OpenSearch Serverless: An on-demand serverless configuration for Amazon OpenSearch Service used as a vector store.\nAmazon Bedrock: Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG) and build agents that execute tasks using your enterprise systems and data sources.\nThe solution is shown in the following figure:\nArchitecture walkthrough Charter-derived documents are processed in an on-premises script in preparation for uploading.\nFiles are sent to AWS using AWS Direct Connect.\nThe script chunks the documents and calls an embedding model to produce the document embeddings. It then stores the embeddings in an OpenSearch vector database for retrieval by our application. Clario uses an Amazon Titan Text Embeddings model offered by Amazon Bedrock. Each chunk is called to produce an embedding.\nAmazon OpenSearch Serverlessis used as the durable vector store. Document chunk embeddings are stored in an OpenSearch vector index, which enables the application to search for the most semantically relevant documents. Clario also stores attributes for the source document and associated trial to allow for a richer search experience.\nA custom build user interface is the primary access point for users to access the system, initiate generation jobs, and interact with a chat UI. The UI is integrated with the workflow engine that manages the orchestration process.\nThe workflow engine calls the Amazon Bedrock API and orchestrates the business requirement specification document generation process. The engine:\nUses a global specification that stores the prompts to be used as input when calling the large language model.\nQueries OpenSearch for the relevant Imaging charter.\nLoops through every business requirement.\nCalls the Claude 3.7 Sonnet large language model from Amazon Bedrock to generate responses.\nOutputs the business requirement specification document to the user interface, where a business requirement writer can review the answers to produce a final document. Clario uses Claude 3.7 Sonnet from Amazon Bedrock for the question-answering and the conversational AI application.\nThe final documents are written to Amazon S3 to be consumed and published by additional document workflows that will be built in the future.\nAn as-needed AI chat agent to allow document-based discovery and enable users to converse with one or more documents.\nBenefits and results By using AWS AI services, Clario has streamlined the complicated BRS generation process significantly. The prototype solution demonstrated the following benefits:\nImproved accuracy: The use of generative AI models minimized the risk of translation errors and inconsistencies, reducing the need for rework and study delays.\nScalability and flexibility: The serverless architecture provided by AWS services allows the solution to scale seamlessly as demand increases, while the modular design enables straightforward integration with other Clario systems.\nSecurity: Clario’s data security strategy revolves around confining all its information within the secure AWS ecosystem using the security features of Amazon Bedrock. By keeping data isolated within the AWS infrastructure, Clario helps ensure protection against external threats and unauthorized access. This approach enables Clario to meet compliance requirements and provide clients with confidence in the confidentiality and integrity of their sensitive data.\nLessons learned The successful implementation of this prototype solution reinforced the value of using generative AI models for domain-specific applications like those prevalent in the life sciences industry. It also highlighted the importance of involving business stakeholders early in the process and having a clear understanding of the business value to be realized. Following the success of this project, Clario is working to productionize the solution in their Medical Imaging business during 2025 to continue offering state-of-the-art services to its customers for best quality data and successful clinical trials.\nConclusion The collaboration between Clario and AWS demonstrated the potential of AWS AI and machine learning (AI/ML) services and generative AI models, such as Anthropic’s Claude, to streamline document generation processes in the life sciences industry and, specifically, for complicated clinical trial processes. By using these technologies, Clario was able to enhance and streamline the BRS generation process significantly, improving accuracy and scalability. As Clario continues to adopt AI/ML across its operations, the company is well-positioned to drive innovation and deliver better outcomes for its partners and patients.\nAbout the Authors Kim Nguyen serves as the Sr Director of Data Science at Clario, where he leads a team of data scientists in developing innovative AI/ML solutions for the healthcare and clinical trials industry. With over a decade of experience in clinical data management and analytics, Kim has established himself as an expert in transforming complex life sciences data into actionable insights that drive business outcomes. His career journey includes leadership roles at Clario and Gilead Sciences, where he consistently pioneered data automation and standardization initiatives across multiple functional teams. Kim holds a Master’s degree in Data Science and Engineering from UC San Diego and a Bachelor’s degree from the University of California, Berkeley, providing him with the technical foundation to excel in developing predictive models and data-driven strategies. Based in San Diego, California, he leverages his expertise to drive forward-thinking approaches to data science in the clinical research space.\nShyam Banuprakash serves as the Senior Vice President of Data Science and Delivery at Clario, where he leads complex analytics programs and develops innovative data solutions for the medical imaging sector. With nearly 12 years of progressive experience at Clario, he has demonstrated exceptional leadership in data-driven decision making and business process improvement. His expertise extends beyond his primary role, as he contributes his knowledge as an Advisory Board Member for both Modal and UC Irvine’s Customer Experience Program. Shyam holds a Master of Advanced Study in Data Science and Engineering from UC San Diego, complemented by specialized training from MIT in data science and big data analytics. His career exemplifies the powerful intersection of healthcare, technology, and data science, positioning him as a thought leader in leveraging analytics to transform clinical research and medical imaging.\nJohn O’Donnell is a Principal Solutions Architect at Amazon Web Services (AWS) where he provides CIO-level engagement and design for complex cloud-based solutions in the healthcare and life sciences (HCLS) industry. With over 20 years of hands-on experience, he has a proven track record of delivering value and innovation to HCLS customers across the globe. As a trusted technical leader, he has partnered with AWS teams to dive deep into customer challenges, propose outcomes, and ensure high-value, predictable, and successful cloud transformations. John is passionate about helping HCLS customers achieve their goals and accelerate their cloud native modernization efforts.\nPraveen Haranahalli is a Senior Solutions Architect at Amazon Web Services (AWS) where he provides expert guidance and architects secure, scalable cloud solutions for diverse enterprise customers. With nearly two decades of IT experience, including over ten years specializing in Cloud Computing, he has a proven track record of delivering transformative cloud implementations across multiple industries. As a trusted technical advisor, Praveen has successfully partnered with customers to implement robust DevSecOps pipelines, establish comprehensive security guardrails, and develop innovative AI/ML solutions. Praveen is passionate about solving complex business challenges through cutting-edge cloud architectures and helping organizations achieve successful digital transformations powered by artificial intelligence and machine learning technologies.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Cleeng’s ChurnIQ AI-ssistant: Subscriber Analytics Powered by Amazon Bedrock This blog is co-authored by Jakub Abramczyk, Data Scientist, Sebastian Boruch, ChurnIQ Product Manager, and Kirstin White, Growth Marketing Manager from Cleeng.\nIn the competitive direct-to-consumer (D2C) streaming market, understanding subscriber behavior is essential for increasing engagement, optimizing content, and reducing customer churn. According to Forbes, acquiring new customers costs 5-7 times more than retaining existing ones. Customer retention is crucial for digital subscription businesses across streaming media, sports, e-learning, wellness, and podcasting. Success depends on analyzing campaign effectiveness, identifying at-risk customers, and making data-driven decisions to maintain competitiveness and maximize revenue.\nTraditionally, extracting insights from subscriber data has required specialized data analysts and business intelligence (BI) teams. This dependency frequently creates bottlenecks, delaying critical time-sensitive responses to potential churn risks. Expanding access to subscriber trend data with a generative AI-based assistant is how Amazon Web Services (AWS) Partner Cleeng addresses this challenge.\nCleeng has built ChurnIQ AI-ssistant, powered by Amazon Bedrock. This generative AI solution enables teams to analyze complex subscriber data through natural language prompts, eliminating the dependency for deep analytical expertise. Users can generate data visualizations and uncover trends instantly, empowering marketing, product, and retention teams with self-service analytics capabilities.\nWe’ll explore how Cleeng built the ChurnIQ AI-ssistant using AWS services (including Amazon Bedrock, AWS Lambda, and Amazon DynamoDB) to transform complex subscriber data into actionable insights for faster, more effective decision-making.\nAbout ChurnIQ AI-ssistant Figure 1: A screenshot of ChurnIQ AI-ssistant interface.\nThe ChurnIQ AI-ssistant is a new, generative AI-based component of ChurnIQ, the analytics product within Cleeng’s Subscriber Retention Management® solution. Cleeng’s solution unifies the entire subscriber experience across web and mobile apps from a single platform, empowering D2C companies to simplify subscription management and maximize customer lifetime value.\nTo help platform teams combat user subscriber churn, ChurnIQ AI-ssistant features predefined queries for common questions, easy chart customization, and seamless integration with ChurnIQ Studio. The platform provides comprehensive insights across the entire subscriber lifecycle, from sign-ups and trial conversions to renewals and churn.\nImportant metrics include transaction volume, revenue by offer and country, customer churn rates, customer lifetime value, subscription changes, and acquisition channels. These metrics help companies understand trends in platform performance, revenue trends, and audience behavior.\nTo generate a chart, users simply enter their question to the ChurnIQ AI-ssistant chatbot interface and are returned quick and accurate answers with visual data charts. These responses and charts give insights about subscriber behavior, churn rates, and revenue. ChurnIQ AI-ssistant then analyzes the data to spot concerning trends, and activates the necessary teams to dive deeper to remediate issues.\nBesides allowing the user to choose their own question, the tool also includes predefined common queries related to user churn, such as:\nWhat are the primary reasons for customer churn?\nHow many customers canceled their subscriptions this week?\nWhat’s the churn rate among subscribers on the monthly plan?\nWhat is the number of returning customers among our new subscribers?\nWhat percentage of new subscriptions come from returning customers?\nAWS architecture overview Figure 2: The AWS architecture of Cleeng’s ChurnIQ AI-ssistant.\nChurnIQ AI-ssistant leverages AWS services like Amazon Bedrock, a fully managed service that offers a choice of industry leading foundation models (FMs) along with a broad set of capabilities needed to build generative AI applications.\nHere is a walkthrough of the underlying AWS services used and the flow of the user’s prompt:\nAn end-user opens the ChurnIQ AI-assistant and asks a natural language query about the data, such as, “Give me a breakdown of churned subscribers by country.”\nThe request is sent to an Amazon API Gateway endpoint that invokes an AWS Lambda function.\nThe AWS Lambda function queries an Amazon DynamoDB table to check if the response for the model already exists. Prompts are cached within DynamoDB for six hours before being overwritten; enabling faster responses and reduced cost to run an identical inference request.\nFor a response that does not already exist, an AWS Lambda function parses the requests, and calls the Amazon Bedrock API with the prompt.\nAmazon Bedrock, running Anthropic’s Claude foundation model, interprets the natural request and evaluates against Amazon Bedrock Guardrails settings to prevent the chatbot from responding to requests not related to ChurnIQ functionality. Amazon Bedrock generates the necessary schema to query the relevant data stores within ChurnIQ by leveraging pre-programmed prompt engineering to automatically select the relevant columns and filters for the requested data.\nThe ChurnIQ AI-ssistant assembles the dashboard for the user to review. The user can customize the chart by modifying filters, metrics, and dimensions. The user can optionally save the chart into their personal folder for later reference.\nFinally, within the AI-ssistant interface, the end-user is presented with the ability to indicate positive or negative feedback on the response. That feedback request is passed through an Amazon API Gateway endpoint, parsed by a Lambda function, and then written to a DynamoDB table. The feedback enables the Cleeng product team to implement additional refinements to the prompt tuning and functionality over time.\nOutcomes and benefits The Cleeng team chose Amazon Bedrock due to the ease of configuration and straightforward integration between Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. This integration streamlined the development process for the ChurnIQ AI-ssistant. Additionally, the ability to utilize the Amazon Bedrock playgrounds to compare different models and responses accelerated the process of selecting the best model for the job.\nChurnIQ AI-ssistant was privately evaluated with customers prior to launch, and the following benefits were observed:\nInstant, accurate answers to critical questions about subscriber behavior, churn rates, and revenue trends.\nActionable insights on the most successful campaigns and highest-risk customer segments.\nDiscoverable campaign ideas based on subscriber behavior to inform opportunities to improve satisfaction and loyalty.\nReduced data analyst and BI team backlog, enabling these teams to focus on high-value tasks.\nConclusion We showcased how Cleeng utilized Amazon Bedrock to integrate their ChurnIQ AI-ssistant to provide digital subscription platforms improved predictive analytics. These unified insights can proactively reduce churn and optimize customer lifetime value.\nLearn more about Cleeng’s ChurnIQ solution and the AI-ssistant by view an interactive demo or contact their Sales Team.\nCheck out more AWS Partners or contact an AWS Representative to know how we can help accelerate your business.\nFurther reading Getting started with Amazon Bedrock\nGuidance for Customer Data Analytics on AWS\nAWS Media \u0026amp; Entertainment Direct-to-Consumer \u0026amp; Streaming\nJason O\u0026rsquo;Malley\nJason O’Malley is a Sr. Partner Solutions Architect at AWS supporting partners architecting media, communications, and technology industry solutions. Before joining AWS, Jason spent 13 years in the media and entertainment industry at companies including Conan O’Brien’s Team Coco, WarnerMedia, and Media.Monks. Jason started his career in television production and post-production before building media workloads on AWS. When Jason isn’t creating solutions for partners and customers, he can be found adventuring with his wife and son, or reading about sustainability.\nClaudio Medeiros\nClaudio Medeiros is a Principal Solutions Architect at AWS with over 14 years of experience helping media and telecom companies innovate and build next-generation video platforms.\nMaggie Osude\nMaggie Osude is a Solutions Architect at AWS, helping enterprises optimize and scale their cloud infrastructure. She drives innovation in media workflows, AI/ML, and cloud efficiency, enabling customers to build scalable, high-performing solutions.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Cloud cost savings: 10 tips for academic institutions When using cloud services, one of the key benefits is the ability to match resources to demand – and avoid purchasing resources above what is needed. This is in contrast to the traditional model of purchasing equipment on-premises, where resources are typically sized for anticipated peak usage across the lifespan of the equipment. This fundamental concept of the cloud can sometimes be overlooked when organizations first migrate to the cloud, and they end up behaving as if they are still locked into a fixed infrastructure model.\nFor academic institutions leveraging cloud services, it is important to proactively manage and optimize cloud costs. Though the following content is geared toward higher education users, many of the points below are helpful to others:\nHere are 10 tips to help control cloud costs using Amazon Web Services (AWS):\n1. Set up AWS budget alerts Utilize AWS Budgets to set up alerts, especially for anomalous spending. This will help you stay on top of usage and avoid unexpected cost spikes.\nThis is especially helpful if you are taking advantage of services on the Free Tier and you want to ensure that you aren’t surprised upon expiry.\nGuidance on how to create a budget can be found here.\n2. Right-size resources Identify underutilized resources, such as over-provisioned Amazon Elastic Cloud Compute (Amazon EC2) instances, Amazon Relational Database Service (Amazon RDS) instances, or Amazon Elastic Block Store (Amazon EBS) volumes. Consider downsizing these resources or exploring alternative instance types like Graviton-based EC2 instances to optimize costs. For those enrolled in Business or Enterprise levels of AWS Support, AWS Trusted Advisor can provide recommendations for right-sizing.\nAWS Compute Optimizer is also a tool that can be used to evaluate EC2 instances, Amazon EC2 Auto Scaling groups, EBS volumes, AWS Lambda functions, RDS databases, and Amazon Elastic Container Service (Amazon ECS) services on AWS Fargate. After enrolling in Compute Optimizer, one can then get recommendations from Compute Optimization Hub found on the “Billing and Cost Management” page within your AWS Console.\nAs resources change over time, ongoing monitoring is important.\n3. Leverage EC2 Spot Instances For fault-tolerant, batch-oriented workloads common in academic computing, EC2 Spot Instances can provide significant cost savings of up to 90% compared to on-demand pricing.\n4. Pause instances after hours Implement auto-scaling policies to shut down EC2 instances during non-business hours, and then restart them when needed. This can help reduce costs for resources that are not needed 24/7. Another method to accomplish this strategy is to implement Instance Scheduler.\n5. Utilize Amazon Simple Storage Service (Amazon S3) lifecycle policies Optimize your S3 storage costs by implementing lifecycle policies to automatically move lesser-used objects to cheaper storage tiers, such as Amazon S3 Glacier Instant Retrieval or S3 Glacier Deep Archive.\nAmazon S3 Storage Lens and Amazon S3 Analytics are tools that can be used to analyze usage of S3 objects and can be helpful with creating lifecycle policies for your S3 storage objects.\n6. Right-size backup retention policies Review your backup retention policies, especially for non-critical or non-production workloads. Reducing unnecessary backup retention can lead to cost savings.\nProduction workloads may have regulatory retention requirements (e.g., 3 or 7 years). For simplicity purposes, it may be easiest to have non-production workloads with the exact same retention, but from a cost point of view, it would be excessive to retain backups that will never be restored and no reason to preserve them for the same duration.\n7. Consider Amazon S3 Intelligent-Tiering Amazon S3 Intelligent-Tiering automatically moves objects between access tiers based on usage patterns, ensuring you’re always using the most cost-effective storage tier.\n8. Delete orphaned storage Identify and delete any unattached EBS volumes, snapshots, or incomplete S3 uploads that are no longer needed. See the associated links which will point to the different methods.\nUsing Amazon Data Lifecycle Manager to manage EBS snapshot volumes is a best practice.\n9. Aggregate accounts under fewer payer accounts For institutions with multiple AWS accounts, consider consolidating them under fewer payer accounts. This can unlock benefits like the Global Data Egress Waiver for higher education customers, as well as potential network resource savings.\nFor those who cannot take advantage of the Global Data Egress Waiver, consider using CloudFront to reduce network traffic out to the Internet from publicly facing web pages.\n10. Utilize AWS Savings Plans and Reserved Instances For consistent, long-term usage of resources like EC2, Lambda, Amazon Sagemaker AI, and RDS consider Savings Plans or Reserved Instances to lock in discounted pricing for 1-3 years. From the menus on the “Billing and Cost Management” page within the AWS Console, there are Savings Plan and Reservation recommendations based on your workloads. One will want to proceed with right-sizing before this step, so that the recommendations can be based on one’s adjusted workloads. Savings Plans and Reserved Instances can provide up to 72% discounts compared to On-Demand pricing.\nBonus #1: Tax Exemption Many public sector entities qualify for tax exemption. Typically, this needs to be configured on the billing console for each of one’s member accounts. For more details, see this link.\nBonus #2: No cost education and support resources Want to learn about AWS Services? Sign up for a Skill Builder account. There are paid and no cost options. Watch videos on the AWS Twitch Channel or the AWS YouTube Channel. Read additional content on AWS Blogs.\nNeed Support? Ask a question or explore prior answers on the AWS Community site: re:Post. Questions are answered by AWS Specialists, Generative AI automation, and other members of the community.\nAdditionally, for those with support agreements, one can open a case from their console to get general guidance or specific assistance.\nFinally, if you want to learn more about cost optimization, see the AWS Well-Architected Cost Optimization Workshop. This workshop focuses on the Cost Optimization pillar and is designed to provide hands-on guidance to explain best practices.\nAnd of course, check with your AWS account team to further explore options to run workshops for your campus. If your campus has an Enterprise Support agreement, your assigned Technical Account Manager can provide guidance as well.\nJim Surlow\nJim is a senior technical account manager (TAM) and enterprise support lead with over 35 years of IT experience. His team of TAMs support Higher Education AWS customers in the western United States. He also a member of both the AWS Storage and Education Technical Field Communities.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “​AI/ML/GenAI on AWS” Event Objectives To understand the landscape of AI on AWS, ranging from specialized ML services to the latest advancements in Agentic AI and real-time voice frameworks. Speakers Dinh Le Hoang Anh – Cloud Engineer Trainee Key Highlights AWS AI/ML Services Overview ​Amazon SageMaker – End-to-end ML platform\n​Data preparation and labeling\n​Model training, tuning, and deployment\n​Integrated MLOps capabilities\n​Live Demo: SageMaker Studio walkthrough\nGenerative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide\n​Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning\n​Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration\n​Bedrock Agents: Multi-step workflows and tool integrations\n​Guardrails: Safety and content filtering\n​Live Demo: Building a Generative AI chatbot using Bedrock\nKey Takeaways The \u0026ldquo;Chasm\u0026rdquo; is Real: There is a significant gap between building a cool AI demo (POC) and a production-ready agent. Issues like latency, security, and \u0026ldquo;hallucination\u0026rdquo; governance must be addressed using robust platforms like Bedrock AgentCore.\nSpecialization over Generalization: For specific tasks (like finding defects in machinery or analyzing customer sentiment), using specialized services like Lookout or Comprehend is often more efficient than building custom models from scratch.\nMultimodal Agents are the Future: Frameworks like Pipecat allow developers to build assistants that can see and hear in real-time, moving beyond text-only interfaces.\nApplying to Work Cost Estimation: When proposing AI solutions, we must account for specific pricing models (e.g., Personalize charges by training hour and inference, while Comprehend charges by character).\nTool Selection: Use LangChain or CrewAI for rapid prototyping of agents, but consider Bedrock AgentCore for enterprise-grade security and governance when moving to production.\nAnomaly Detection: Evaluate Amazon Lookout for internal operations monitoring to automate defect detection.\nSome event photos "},{"uri":"https://ngocLong216.github.io/fcj_report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “DepOps on AWS” Event Objectives To master the AWS DevOps toolkit, focusing on shifting from manual operations to automated \u0026ldquo;Infrastructure as Code,\u0026rdquo; understanding container orchestration, and implementing self-healing observability. Speakers Truong Quang Tinh – AWS Communit Builder, Platform Engineer - Tymex Kha Van Tran Vi Long Quy Nghiem Key Highlights DevOps Mindset ​Recap of AI/ML session\n​DevOps culture and principles\n​Benefits and key metrics (DORA, MTTR, deployment frequency)\nAWS DevOps Services – CI/CD Pipeline ​Source Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based)\n​Build \u0026amp; Test: CodeBuild configuration, testing pipelines\n​Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates\n​Orchestration: CodePipeline automation\n​Demo: Full CI/CD pipeline walkthrough\nInfrastructure as Code (IaC) AWS CloudFormation: Templates, stacks, and drift detection\n​AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support\n​Demo: Deploying with CloudFormation and CDK\n​Discussion: Choosing between IaC tools\nContainer Services on AWS ​Docker Fundamentals: Microservices and containerization\n​Amazon ECR: Image storage, scanning, lifecycle policies\n​Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration\n​AWS App Runner: Simplified container deployment\n​Demo \u0026amp; Case Study: Microservices deployment comparison\nMonitoring \u0026amp; Observability ​CloudWatch: Metrics, logs, alarms, and dashboards\n​AWS X-Ray: Distributed tracing and performance insights\n​Demo: Full-stack observability setup\n​Best Practices: Alerting, dashboards, and on-call processes\nDevOps Best Practices \u0026amp; Case Studies ​Deployment strategies: Feature flags, A/B testing\n​Automated testing and CI/CD integration\n​Incident management and postmortems\n​Case Studies: Startups and enterprise DevOps transformations\nKey Takeaways Observability is Active, Not Passive: Modern monitoring isn\u0026rsquo;t just looking at dashboards; it\u0026rsquo;s about configuring CloudWatch to act (restart servers, scale out) when thresholds are breached.\nThe \u0026ldquo;L1\u0026rdquo; vs. \u0026ldquo;L3\u0026rdquo; Trade-off: When using CDK, you have a choice between granular control (L1 Constructs mapping 1:1 to CloudFormation) or high-level abstraction (L3 patterns). Understanding this layer helps in debugging.\nTesting Strategy Shift: For microservices, unit tests aren\u0026rsquo;t enough. We must explicitly test for \u0026ldquo;Connection Failures\u0026rdquo; and \u0026ldquo;API Contracts\u0026rdquo; to prevent cascading failures.\nApplying to Work Self-Healing Infrastructure: Review our current alarms. Can we attach a Lambda function to any of them to automatically fix the issue (e.g., clear a cache) instead of waking up an engineer?\nrototyping: For the next internal tool or MVP, use App Runner instead of EC2/ECS to reduce setup time to near zero.\nStandardization: Adopt CDK with TypeScript for our infrastructure to allow developers to read/write infra definitions in a language they already know.\nSome event photos "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations. 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services - Learn about AWS Console \u0026amp; AWS CLI 09/09/2025 09/09/2025 https://youtu.be/2PQYqH_HkXw?si=AAWaEt0lYjuMHKdz 4 - Create AWS Free Tier account - Setup with Virtual MFA Device - Practice: + Create AWS account + Login with root \u0026amp; IAM user 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/ 5 - Cost Management with AWS Budgets - Practice: + Create Budget by Template + Create Cost Budget 11/09/2025 11/09/2025 https://000007.awsstudygroup.com/vi/ 6 - Learn about AWS support Packages \u0026amp; its types 12/09/2025 12/09/2025 https://000009.awsstudygroup.com/vi/ Week 1 Achievements: Understood what AWS is\nSuccessfully created and configured an AWS Free Tier account.\nExplored different categories of AWS services (Compute, Storage, Networking, Databases, Security, etc.).\nLearned how to use AWS Management Console and AWS CLI effectively.\nEnabled Multi-Factor Authentication (MFA) for better security on AWS account.\nPracticed logging in with both Root and IAM user accounts.\nGained knowledge about AWS Budgets and created cost management budgets using templates and custom options.\nLearned about AWS Support Packages and understood the differences between Basic, Developer, Business, and Enterprise support plans.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand and practice AWS networking services, especially Amazon VPC and its components. Set up and configure network connections in a Hybrid Cloud environment: VPN, DirectConnect, Hybrid DNS. Implement and configure advanced AWS networking features: VPC Peering, Transit Gateway, Network ACL, Load Balancer. Gain experience with CloudFormation to automate network resource deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Virtual Private Cloud (VPC) + Subnet + Route table + ENI, EIP + Endpoint + Internet gateway - VPC Security and Multi-VPC features - VPN - DirectConnect - LoadBalancer - ExtraResources 15/09/2025 15/09/2025 https://youtu.be/O9Ac_vGHquM?si=_eLRx1ohGnWONjq6 3 - Practice Amazon VPC and AWS Site-to-Site VPN connections - Practice: + Create VPC + Deploying Amazon EC2 Instances + Setting Up Site-to-Site VPN Connection in AWS 16/09/2025 16/09/2025 https://000003.awsstudygroup.com 4 - Set up Hybrid DNS with Route 53 Resolver - Practice: + Initialize CloudFormation Template + Connecting to RDGW + Deploy Microsoft AD + Setup DNS 17/09/2025 17/09/2025 https://000010.awsstudygroup.com 5 - Setting up VPC Peering - Cross-Peer DNS - Network ACL 18/09/2025 18/09/2025 https://000019.awsstudygroup.com/vi/ 6 - Set up AWS Transit Gateway - Practice: + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 19/09/2025 19/09/2025 https://000020.awsstudygroup.com/ Week 2 Achievements: Explored AWS VPC and its components: Subnet, Route Table, ENI, EIP, Endpoint, Internet Gateway. Understood VPC security concepts and Multi-VPC features. Learned the basics of VPN, DirectConnect, Load Balancer, and extra resources. Practiced creating a VPC and deploying EC2 instances inside it. Set up a Site-to-Site VPN connection between on-premises and AWS. Improved hands-on skills with AWS virtual networking. Configured Hybrid DNS with Route 53 Resolver. Deployed Microsoft AD using CloudFormation and set up DNS. Successfully connected to RDGW. Configured VPC Peering between multiple VPCs. Set up Cross-Peer DNS for domain resolution across VPCs. Managed network security using Network ACL. Deployed and configured AWS Transit Gateway. Created Transit Gateway Attachments and Route Tables. Added Transit Gateway routes to VPC Route Tables. Completed a multi-VPC network connectivity model. "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about AWS compute and storage services: EC2 (VM), Lightsail, EFS/FSx. Explore AWS Application Migration Service (MGN). Deploy AWS Backup and practice creating backup plans, setting up notifications, and testing restores. Get familiar with AWS Storage Gateway, practice creating file shares, and mount them on an on-premises machine. Start with Amazon S3: create a bucket, enable static website hosting, configure public access and object permissions, test the website, and accelerate it with CloudFront. Conduct architectural research for the AWS team project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about compute VM on AWS - Amazon Lightsail - Amazon EFS/FSX - AWS Application Migration Service (MGN) 22/09/2025 22/09/2025 https://youtu.be/-t5h4N6vfBs?si=7eGLk3Bf5ol4M96x 3 - Deploy AWS Backup to the System - Practice: + Create backup plan + Set up notifications + Test restore 23/09/2025 23/09/2025 https://000013.awsstudygroup.com 4 - Using File Storage Gateway - Practice: + Create Storage Gateway + Create File Shares + Mount File shares on On-premises machine 24/09/2025 24/09/2025 https://000024.awsstudygroup.com/ 5 - Starting with Amazon S3 - Practice: + Create S3 bucket + Enable static website feature + Configuring public access block + Configuring public objects + Test website + Accelerate Static Websites with Cloudfront 25/09/2025 25/09/2025 https://000057.awsstudygroup.com/vi/ 6 - Architectural research for AWS team project 26/09/2025 26/09/2025 Week 3 Achievements: Learned and practiced the following AWS services:\nCompute: EC2 (VM), Amazon Lightsail Storage: Amazon EFS, Amazon FSx, Amazon S3, AWS Storage Gateway Migration: AWS Application Migration Service (MGN) Backup: AWS Backup (plan creation, notifications, restore test) Successfully deployed AWS Backup: created a backup plan, set up notifications, and tested restore.\nCreated and mounted a File Storage Gateway on an on-premises machine.\nConfigured Amazon S3 static website hosting and accelerated it with CloudFront.\nCompleted initial architectural research for the AWS team project.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Gain a solid understanding of core AWS service groups: Compute, Storage, Networking, and Database. Learn to use both the AWS Management Console and the AWS CLI effectively. Explore storage-related services such as S3, Amazon Storage Gateway, Snow Family, AWS Backup, and Amazon FSx. Practice VM Import/Export by importing virtual machines to AWS and exporting instances from AWS. Participate in the AWS GenAI Builder Club event and summarize key insights. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about storage service on AWS + S3 + Amazon Storage Gateway + Snow Family - Disaster Recovery on AWS - AWS Backup\n29/09/2025 29/09/2025 https://youtu.be/hsCfP0IxoaM?si=IChJwQVIszhhCfZC 3 - VM Import/Export - Practice: + Import virtual machine to AWS + Export instance from AWS 30/09/2025 30/09/2025 https://000014.awsstudygroup.com 4 - Write proposal for team project 01/10/2025 01/10/2025 5 - Amazon FSx for Windows File Server\n- Practice: + Create new file shares + Manage user sessions and open files 02/10/2025 02/10/2025 https://000025.awsstudygroup.com/vi/ 6 - Participated in the AWS GenAI Builder Club event and noted the key ideas. 03/10/2025 03/10/2025 Week 4 Achievements: Understood the structure and purpose of the main AWS service groups: Compute, Storage, Networking, and Database. Created and configured an AWS Free Tier account for hands-on learning. Became proficient in navigating the AWS Management Console and locating essential services. Installed and configured the AWS CLI, including Access Key, Secret Key, and default region setup. Used the AWS CLI to execute fundamental operations: Verified account and configuration information Listed available AWS regions Viewed and managed EC2 instances and key pairs Retrieved information about running services Practiced VM import/export between local virtual machines and AWS successfully. Implemented storage and backup solutions using S3, AWS Backup, and FSx. Participated in the AWS GenAI Builder Club event and summarized the main ideas presented. Strengthened the ability to manage AWS resources via both the Console and CLI in parallel. "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Strengthen understanding of AWS security and cost optimization through IAM, KMS, Security Hub, and tagging practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Share Responsibility Model - AWS IAM - AWS Cognito - AWS SSO - AWS KMS 06/10/2025 06/10/2025 https://youtu.be/tsobAlSg19g?si=DM-IVD6B1fDl9-1u 3 - Get started with AWS Security Hub - Practice: + Enable Security Hub + Score for each set of criteria 07/10/2025 07/10/2025 https://000018.awsstudygroup.com 4 - Optimizing EC2 Costs with Lambda 08/10/2025 08/10/2025 https://000022.awsstudygroup.com/vi/ 5 - Manage Resources Using Tags and Resource Groups - Practice: + Using Tags + Create a Resource Group 09/10/2025 09/10/2025 https://000027.awsstudygroup.com/vi/ 6 - Encrypt at rest with AWS KMS 10/10/2025 10/10/2025 https://000033.awsstudygroup.com/vi/ Week 5 Achievements: Gained hands-on experience with AWS IAM, Cognito, and KMS.\nEnabled and analyzed Security Hub findings.\nPracticed cost optimization using Lambda and managed resources with tags and groups.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Deepen understanding of AWS database and serverless services, including RDS, Aurora, ElastiCache, Redshift, and Lambda.\nPractice database setup, access control, and service integration using IAM and DMS.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Database Concepts - Amazon RDS - Amazon Aurora - Amazon ElastiCache - Amazon Redshift 13/10/2025 13/10/2025 https://youtu.be/OOD2RwWuLRw?si=M2-tSSZwtOnM9L8- 3 - Get started with Amazon Relational Database Service - Practice: + Create RDS database instance + Amazon RDS Backup and Restore 14/10/2025 14/10/2025 https://000005.awsstudygroup.com 4 - Granting authorization for an application to access AWS services with an IAM role 15/10/2025 15/10/2025 https://000048.awsstudygroup.com/vi/ 5 - Getting Started with AWS Lambda Functions - Practice: + Image Resizing with AWS Lambda + Writing Data to Amazon DynamoDB 16/10/2025 16/10/2025 https://000078.awsstudygroup.com/vi/ 6 - DMS - Introduction DMS and Writing Lambda Functions - Practice: + Create Lambda functions + Test lambda functions 17/10/2025 17/10/2025 https://000133.awsstudygroup.com/vi/ Week 6 Achievements: Gained practical experience with Amazon RDS, including instance creation, backup, and restore.\nImplemented IAM roles for secure service access.\nBuilt and tested AWS Lambda functions for image resizing and DynamoDB integration.\nExplored AWS DMS fundamentals and practiced Lambda-based data integration.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Analyze cost and performance using AWS Glue and Athena.\nBuild and test a serverless frontend integrated with API Gateway.\nImplement authentication with Amazon Cognito.\nUnderstand API Gateway integration request/response.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Cost and performance analysis with AWS Glue and Amazon Athena - Analysis of cost and usage performance 20/10/2025 20/10/2025 https://000040.awsstudygroup.com/ 3 - Serverless - Build Frontend to call API Gateway - Practice: + Deploy FE, Lambda + Config API Gateway + Test API with Postman, FE 21/10/2025 21/10/2025 https://000079.awsstudygroup.com 4 - Authentication with Amazon Cognito 22/10/2025 22/10/2025 https://000081.awsstudygroup.com/vi/ 5 - Create Amazon Cognito User Pool 23/10/2025 23/10/2025 https://youtu.be/S1X5QxBoX4M?si=wSddLWXGzbsceVS3 6 - Learn about Integration Request/Respond in API gateway 24/10/2025 24/10/2025 https://youtu.be/q-0JoYFag7k?si=E343RKXlnw9f456A Week 7 Achievements: Completed cost and performance analysis.\nDeployed frontend and Lambda; successfully tested APIs.\nConfigured Cognito and created a User Pool.\nGained understanding of API Gateway integration mechanisms.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn and understand core AWS Architecture Pillars: Secure Architectures (IAM, MFA, SCP, KMS, Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager) Resilient Architectures (Multi-AZ/Region, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore) High-Performing Architectures (Compute scaling, Storage options, Caching, CloudFront) Cost-Optimized Architectures (Cost Explorer, Budgets, Saving Plans, NAT Gateway optimization, Storage tiering) Implement and deploy API Gateway Proxy Resource Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Secure Architectures + IAM, MFA, SCP, KMS + Security Groups NACLs, GuardDuty, Shield, WAF, Secrets Manager 27/10/2025 27/10/2025 3 - Learn about Resilient Architectures + Multi AZ/Region, Auto Scaling\u0026hellip; + Route 53, Load Balancing, Backup \u0026amp; Restore 28/10/2025 28/10/2025 4 Learn about High-Performing Architectures + Compute scaling (EC2, Lambda, Fargate) + Storage (S3, EFS, EBS), Caching, CloudFront 29/10/2025 29/10/2025 5 - Cost-Optimized Architectures + Cost Explorer, Budgets, Saving Plans Nat gateway, Storage Tiering 30/10/2025 30/10/2025 6 - Learn about Proxy Resource \u0026amp; Deploy Proxy Resource on API gateway 31/10/2025 31/10/2025 https://youtu.be/zZzHTHs72Sk?si=qhdd4v0mADIh3MJ0 Week 8 Achievements: Completed learning modules for: Secure AWS Architectures and core security services Resilient Architectures including multi-AZ/region strategies High-Performance Architectures with compute, storage, and caching Cost Optimization techniques across AWS services Successfully deployed API Gateway Proxy Resource following tutorial Completed all planned tasks from Day 2 to Day 6 on schedule "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn and deepen understanding of API Gateway concepts: Path Parameters in API Gateway Query String Parameters Lambda Integration Proxy Usage Plan \u0026amp; API Key Configuring CloudWatch Logs for API Gateway Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Path Parameters in API gateway 03/11/2025 3/11/2025 https://youtu.be/5cMHc1kiq2M?si=zip07VzxdNcjCQpF 3 - Learn about Query String in API gateway 04/11/2025 04/11/2025 https://youtu.be/BZbF5n39Xnc?si=QHhm0xDo36cuOFcI 4 - Using Lambda Integration Proxy 05/11/2025 05/11/2025 https://youtu.be/369Em-gKTlE?si=veLKX3EbbLW-S6uO 5 - Learn about Usage Plan and API key 06/11/2025 06/11/2025 https://youtu.be/rMG5-pklJO0?si=n14pWvOIGxTkbbw0 6 - Learn to config Amazon Cloudwatch Logs for API gateway 07/11/2025 07/11/2025 https://youtu.be/OIR2I4CC4N8?si=_5A7an6gxSFcuepv Week 9 Achievements: Completed learning activities on: Path Parameters and Query String Parameters in API Gateway Proper use of Lambda Integration Proxy Creating and configuring Usage Plans and API Keys Enabling and configuring CloudWatch Logs for API Gateway Successfully completed all planned tasks from Day 2 to Day 6 on schedule "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"My worklog carried out over about 3 months with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Amazon VPC, Hybrid DNS, and Transit Gateway\nWeek 3: AWS compute, storage, backup, migration, and initial architectural research.\nWeek 4: Exploring AWS Storage Solutions and Disaster Recovery\nWeek 5: Enhancing Security and Optimizing Costs on AWS\nWeek 6: Hands-on Exploration of AWS Database and Serverless Services\nWeek 7: Serverless Architecture, API Integration, and Authentication\nWeek 8: AWS Architecture Deep Dive \u0026amp; API Gateway Success\nWeek 9: API Gateway Advanced Concepts \u0026amp; Logging\nWeek 10: Serverless CRUD, Excel Upload \u0026amp; API Gateway Models\nWeek 11: AWS DevOps, API Flexibility \u0026amp; Traffic Routing\nWeek 12: CloudFront Pricing, Docker Workflow \u0026amp; Auto Scoring with Lambda\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.2-prerequisite/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites for InsightHR Workshop Before starting this workshop, ensure you have the following tools and accounts set up.\n1. AWS Account You\u0026rsquo;ll need an AWS account with appropriate permissions to create and manage resources.\nRequired AWS Services Access:\nIAM (Identity and Access Management) DynamoDB Lambda API Gateway S3 CloudFront Cognito Bedrock CloudWatch Route53 (optional, for custom domain) Estimated Costs: $2-5/month during development\nIf you\u0026rsquo;re using AWS Free Tier, many services in this workshop are covered. However, some services like Bedrock may incur small charges.\n2. AWS CLI Install and configure the AWS Command Line Interface.\nInstallation:\nWindows:\nmsiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\ncurl \u0026#34;https://awscli.amazonaws.com/AWSCLI2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configuration:\naws configure Enter your:\nAWS Access Key ID AWS Secret Access Key Default region (e.g., ap-southeast-1) Default output format (e.g., json) Verify Installation:\naws --version # Expected output: aws-cli/2.x.x Python/3.x.x ... 3. Node.js and npm Required for frontend development.\nMinimum Version: Node.js 18+\nInstallation:\nDownload from nodejs.org or use a version manager:\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js nvm install 18 nvm use 18 Verify Installation:\nnode --version # Expected: v18.x.x or higher npm --version # Expected: 9.x.x or higher 4. Python Required for Lambda function development.\nMinimum Version: Python 3.11+\nInstallation:\nDownload from python.org or use your system\u0026rsquo;s package manager.\nVerify Installation:\npython --version # or python3 --version # Expected: Python 3.11.x or higher Install pip (if not included):\npython -m ensurepip --upgrade 5. Text Editor / IDE Choose your preferred development environment:\nRecommended: Visual Studio Code\nDownload from code.visualstudio.com Install recommended extensions: AWS Toolkit Python ESLint Prettier Tailwind CSS IntelliSense Alternatives:\nPyCharm Sublime Text Atom WebStorm 6. Git (Optional but Recommended) For version control and accessing code repositories.\nInstallation:\nDownload from git-scm.com\nVerify Installation:\ngit --version # Expected: git version 2.x.x 7. Required Knowledge JavaScript/TypeScript:\nBasic syntax and ES6+ features Async/await and Promises React fundamentals (components, hooks, state) Python:\nBasic syntax and data structures Functions and error handling Working with JSON AWS Concepts:\nBasic understanding of cloud computing Familiarity with AWS Console Understanding of serverless architecture (helpful but not required) Web Development:\nHTML/CSS basics REST API concepts HTTP methods (GET, POST, PUT, DELETE) 8. AWS Account Setup Create IAM User For security best practices, create an IAM user instead of using root credentials:\nSign in to AWS Console\nNavigate to IAM service\nClick \u0026ldquo;Users\u0026rdquo; → \u0026ldquo;Add users\u0026rdquo;\nEnter username (e.g., insighthr-admin)\nSelect \u0026ldquo;Programmatic access\u0026rdquo; and \u0026ldquo;AWS Management Console access\u0026rdquo;\nAttach policies:\nAdministratorAccess (for workshop purposes) Or create a custom policy with required permissions Download credentials (Access Key ID and Secret Access Key)\nConfigure AWS CLI with these credentials\nEnable Required Services Ensure the following services are available in your region:\n✅ AWS Lambda ✅ Amazon DynamoDB ✅ Amazon API Gateway ✅ Amazon S3 ✅ Amazon CloudFront ✅ Amazon Cognito ✅ Amazon Bedrock (check regional availability) ✅ Amazon CloudWatch Recommended Region: ap-southeast-1 (Singapore) - All services are available and latency is good for Southeast Asia.\n9. Bedrock Model Access AWS Bedrock requires explicit model access request.\nSteps to Enable:\nGo to AWS Console → Amazon Bedrock Navigate to \u0026ldquo;Model access\u0026rdquo; in the left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box and click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant for Haiku) Without Bedrock access, the AI chatbot feature won\u0026rsquo;t work. However, you can still complete the rest of the workshop.\n10. Optional: Domain Name If you want to use a custom domain (like insight-hr.io.vn):\nPurchase a domain from a registrar (e.g., Route53, GoDaddy, Namecheap) Have access to DNS management Budget for SSL certificate (free with AWS Certificate Manager) Pre-Workshop Checklist Before proceeding to the next section, verify you have:\nAWS account with admin access AWS CLI installed and configured Node.js 18+ and npm installed Python 3.11+ installed Text editor/IDE set up Git installed (optional) Basic knowledge of JavaScript/TypeScript and Python Understanding of REST APIs AWS Bedrock model access requested Familiarity with AWS Console Troubleshooting AWS CLI Configuration Issues:\n# Check current configuration aws configure list # Test AWS access aws sts get-caller-identity Node.js Version Issues:\n# Check installed versions nvm list # Switch to correct version nvm use 18 Python Version Issues:\n# Check Python path which python3 # Create virtual environment python3 -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate Next Steps Once you\u0026rsquo;ve completed all prerequisites, proceed to Project Architecture to understand the system design.\nAdditional Resources AWS CLI Documentation Node.js Documentation Python Documentation AWS Free Tier AWS Bedrock Documentation "},{"uri":"https://ngocLong216.github.io/fcj_report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS First Cloud AI Journey – Project Plan\n📥 Download Proposal\n[Project team] – [University] – [Project Name]\n[Date]\nTable of Contents\n1 BACKGROUND and motivation 1.1 executive summary\n1.2 PROJECT SUCCESS CRITERIA\n1.3 Assumptions\n2 SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram\n2.2 Technical Plan\n2.3 Project Plan\n2.4 Security Considerations\n3 Activities AND Deliverables\n3.1 Activities and deliverables\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION\n4 EXPECTED AWS COST BREAKDOWN BY SERVICES\n5 TEAM\n6 resources \u0026amp; cost estimates\n7 Acceptance\n1 BACKGROUND AND MOTIVATION 1.1 Executive Summary Organization faces HR evaluation inefficiencies due to manual data handling, lack of transparency in evaluation processes and metrics tracking.\nInsightHR delivers HR automation through flexible evaluation management, automated scoring. AWS provides serverless scalability, cost efficiency, security for sensitive data, and rapid deployment.\nCustom KPI, automated performance scoring, multi-level dashboards, automated notifications, role-based access (Admin/Manager/Employee), multi-tenant support.\nEnd-to-end delivery including Well-Architected design, serverless backend (Lambda, DynamoDB, API Gateway), frontend (S3 + CloudFront), authentication/security (Cognito, IAM), KPI/formula builder, notifications (SNS, SES), CI/CD, monitoring, and knowledge transfer.\n1.2 Project Success Criteria Success is defined by demonstrating a functional MVP that proves the platform\u0026rsquo;s capability to automate HR evaluations and deliver measurable business value.\nFunctional Criteria: Authentication with role-based access (Admin/HR, Manager, Employee)\nHR creates custom KPIs without technical support\nCSV upload triggers automated Lambda scoring\nDashboard displays individual/team performance with charts\nSES sends automated email notifications\nTechnical Criteria:\n99.9%+ uptime\n\u0026lt;300ms API latency (95th percentile)\n95%+ scoring accuracy vs manual calculations\nZero critical security vulnerabilities\nPerformance \u0026amp; Cost:\n~$11.53/month AWS cost\nEnd-to-end workflow (upload → score → visualize) completes in \u0026lt;5 minutes\nBusiness Impact:\nDemonstrates 60%+ HR time reduction potential\nNon-technical users operate KPI builder independently\nDelivery:\nWeek 8: MVP (authentication, KPI/formula management, scoring, basic dashboard)\nWeek 12: Full features (notifications, advanced dashboard)\n1.3 Assumptions 1. Assumptions:\nThe current AWS cost estimate of approximately $11.53/month is accurate for the projected initial load and usage.\nThe required data format and mapping logic for employee performance data can be clearly defined and provided by the HR team for the automated scoring engine.\nThe automated scoring system is trained locally. The technical evaluation files of each team are assessed according to the companies\u0026rsquo; criteria and must follow the format provided by the customer.\n2. Constraints:\nThe project delivery must adhere to the 12-week timeline utilizing the Agile Scrum framework.\nThe solution must be built entirely on serverless AWS services to meet the objectives of scalability, cost efficiency, and reduced operational overhead.\nThe final production AWS cost must remain around the ~$11.53/month target.\n3. Risks:\nData Security/Compliance: Failure to fully understand or implement all of the customer\u0026rsquo;s specific regulatory control validation requirements could impact the project\u0026rsquo;s ability to meet security objectives.\nFeature Creep: Requests for features identified as \u0026ldquo;Out of Scope\u0026rdquo; could derail the 12-week MVP delivery timeline.\n2 SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The InsightHR platform is built on a serverless architecture using AWS services, providing scalability, cost-effectiveness, and high availability, the architecture includes:\nFrontend \u0026amp; Content Delivery: Amazon S3: Hosts the static website and stores user-uploaded files (CSV). S3 Vector to store vectors (embeddings for text-data), S3 Standard for storing raw documents..\nAmazon CloudFront: Distributes static and dynamic content globally with low latency.\nBackend \u0026amp; Compute: AWS Lambda: Executes all business logic, including authentication, custom scoring.\nAmazon API Gateway: Manages APIs as the communication gateway between frontend and backend.\nData Storage: Amazon DynamoDB: Stores structured data such as user/employee information, company KPIs, scoring formulas, and performance evaluation results. Security \u0026amp; Identity: Amazon Cognito: Manages user authentication, registration, and identity workflows.\nAWS IAM: Manages access control and permissions for AWS services.\nAWS KMS: Encrypts sensitive data in DynamoDB and S3.\nMonitoring \u0026amp; Notifications: Amazon CloudWatch \u0026amp; CloudWatch Logs: Monitors Lambda functions, API Gateway, and database access.\nAmazon SNS: Sends notifications (e.g., reminders, result notifications) to employees.\nArchitecture Benefits: Serverless: No server management and automatic scaling.\nCost-Effective: Mostly pay-as-you-go services.\nHigh Availability: Built-in redundancy across AWS regions.\nScalable: Can handle growth from small teams to large enterprises.\nFlexible: Easy to modify and extend functionality.\nProposed Architecture Diagram: Tools Proposed for This Project: Amazon CloudFront: For global content delivery and caching of static and dynamic web content.\nAmazon S3: To host static web assets and store documents, vector embeddings, and other files processed by the system.\nAmazon API Gateway: To provide a secure RESTful interface, acting as the communication layer between frontend clients and backend services.\nAWS Lambda: To run backend business logic including user dashboard, auto scoring, and data management workflows.\nAmazon DynamoDB: To store application data such as user information, HR records, scoring results, and vector metadata with low-latency performance.\nAmazon Cognito: To manage user authentication, authorization, registration, MFA, and secure access to APIs and frontend applications.\nAWS Identity and Access Management (IAM): To define fine-grained access policies and control permissions between services and users.\nAWS Key Management Service (KMS): To manage encryption keys used for securing sensitive data stored in S3, DynamoDB, and logs.\nAmazon ECR: To store containerized model assets and application dependencies in a secure and version-controlled repository.\nAmazon Simple Email Service (SES): To send automated email notifications such as onboarding alerts and HR communications.\nAmazon Simple Notification Service (SNS): To publish notifications and trigger downstream processes; integrates with email, SMS, and microservices.\nAmazon CloudWatch \u0026amp; CloudWatch Logs: For monitoring performance, logging, tracing, and operational troubleshooting across Lambda, API Gateway.\n2.2 Technical Plan The partner will develop automated deployment scripts using AWS CloudFormation and Infrastructure as Code (IaC) practices.\nThis will allow for quick and repeatable deployments into AWS accounts. Some additional configurations such as WAF rules on CloudFront for enhanced security may require approval and will follow standard DevOps change management processes.\nApplication Feature Implementation:\n1. Authentication \u0026amp; Security Module\nUser Management: Cognito manages user lifecycle Registration, login, password reset workflows\nAccess Control: IAM and RBAC enforce role-based permissions Admin/HR, Manager, and Employee access levels\nAPI Security: API Gateway implements JWT-protected endpoints Token validation before Lambda processing\n2. Administration Module (HR Panel)\nKPI Management: HR creates, edits, and deletes custom metrics Examples: Tasks Completed, Code Quality, Customer Satisfaction Definitions stored in DynamoDB\nAuto scoring by employee’s technical score for each team with ML model.\n3. Core User Functions\nData Upload \u0026amp; Mapping: Upload performance data files (CSV) to DynamoDB\nScoring Engine: Lambda triggered on upload Retrieves active formula from DynamoDB Calculates employee scores Stores results in DynamoDB Flow: Upload → Validate → Map → Calculate → Store\nDashboard: Visualize individual and department performance Line graphs, bar charts, trend analysis\nNotifications: SES sends automated alerts Performance milestones, review reminders, custom triggers\n2.3 Project Plan The partner will adopt the Agile Scrum framework over 12 one-week sprints totaling a 12-week delivery timeline.\nTeam Responsibilities Product Owner: Prioritizes backlog (KPIs, formulas, analytics) Final authority on feature acceptance\nDevelopment Team: Implements Cognito authentication Builds admin portal and formula builder Develops scoring engine and dashboard Integrates SNS with SES notifications via Email.\nQA Personnel: Conducts functional, performance, and security testing Facilitates UAT Ensures compliance and quality standards\nCommunication Cadences Daily Standups (30 min - 1 hr): Progress review and blocker identification\nRetrospectives (Weekly, 1 hr): Process improvement and delivery optimization\nExecutive Updates (Weekly): Written reports on progress, risks, KPIs, roadmap Leadership decisions required\nKnowledge Transfer Sessions conducted by the development team covering AWS serverless fundamentals KPI and formula configuration Data workflows and column-mapping Dashboard navigation and analytics System monitoring (CloudWatch, Cognito, DynamoDB) 2.4 Security Considerations The partner will implement AWS security best practices based on the Well-Architected Framework, prioritizing protection of sensitive HR data while ensuring high operational availability. Security implementation covers five key categories:\n1. Access Control\nCognito manages user identities Enforces strong password policies and MFA support\nIAM implements RBAC Admin/HR access Admin Panel and KPI/Formula configurations Employees view only their own performance data\nAPI Gateway validates JWT tokens Cognito-issued tokens verified before Lambda processing\n2. Infrastructure Security\nServerless architecture reduces attack surface No OS or server patching required\nLambda functions communicate via private AWS networks Only necessary endpoints exposed through API Gateway\n3. Data Protection\nKMS encrypts data at rest DynamoDB and S3 encrypted Data unusable without decryption keys\nTLS/SSL (HTTPS) encrypts data in transit All frontend-backend communication secured\n4. Detection \u0026amp; Monitoring\nCloudWatch Logs captures execution details Lambda and API Gateway activity logged Real-time monitoring and anomaly detection enabled\nAWS Config tracks configuration changes Ensures resource compliance with security objectives\n5. Incident Management\nCloudWatch Alarms trigger automated alerts via SES Failed login threshold breaches Lambda resource anomalies\nSecurity Hub provides consolidated security view Unified compliance findings across AWS environment Simplifies incident identification and response\nAWS CloudTrail and AWS Config will be configured for continuous monitoring of activities and compliance status of resources. The customer will share their regulatory control validation requirements as inputs for the partner to ensure all security objectives are met.\n3 ACTIVITIES AND DELIVERABLES 3.1 Activities And Deliverables NOTE: Some Project Phases overlap each other.\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Phase 1: Foundation \u0026amp; Scoring Model Week 1-8 • Personal infrastructure architecture research • Data generation for local model training • Scoring model build • Finalized personal architecture diagram • Ready dataset for Local Model training • Scoring Model MVP (Minimum Viable Product) 80 Phase 2: Project Setup \u0026amp; Dashboard Week 9-10 • Project Setup with basic functions: IAM Role, CRUD function, Static web • Web UI Demo • Implement Dashboard • Fix Model • Basic IAM Roles configured • Operational CRUD functions • Static website deployed (S3/CloudFront) • Web UI Demo completed • Dashboard displaying data implemented 40 Phase 3: Absence Mgmt \u0026amp; Notification Week 11 • Implement Absent managing • Setup SNS notification system • Operational Absence tracking workflow implemented • SNS notification system fully integrated and tested 15 Phase 4: Integration, Testing \u0026amp; Handover Week 12 • Final Integration • Testing and set up Monitoring • Application components fully integrated • Functional, Performance, and Security Testing completed • Monitoring (CloudWatch) configured and operational • Project Completion Report \u0026amp; Post-implementation support plan delivered 15 3.2 Out Of Scope AI Enhancements AI-Powered Insights:\nWhen sufficient data is available, develop AI models capable of:\nIdentifying performance patterns across teams and departments\nPredicting HR risks (e.g., turnover likelihood, burnout indicators)\nRecommending personalized development plans\nDetecting anomalies in performance data\nSuggesting optimal team compositions\nMachine Learning Features:\nPredictive analytics for workforce planning\nSentiment analysis from employee feedback\nAutomated skill gap analysis\nPerformance trend forecasting\nPublic API Development API Ecosystem:\nBuild a comprehensive API set allowing other internal business systems to automatically push performance data into InsightHR. Integration Targets:\nProject management tools (Jira, Asana, Monday.com)\nCRM systems (Salesforce, HubSpot)\nTime tracking software (Toggl, Harvest)\nCommunication platforms (Slack, Microsoft Teams)\nCode repositories (GitHub, GitLab, Bitbucket)\nBenefits:\nTransform InsightHR into a central HR data processing hub\nCreate a synchronized and comprehensive management ecosystem\nEliminate manual data entry\nReal-time performance tracking\nAdvanced Features Mobile Applications:\niOS and Android native apps\nPush notifications\nOffline capabilities\nMobile-optimized dashboards\nDynamoDB back up\nAdvanced Analytics:\nPredictive modeling\nBenchmarking across industries\nCustom report builder\nData export and API for third-party tools\nCollaboration Features:\nPeer review systems\n360-degree feedback\nGoal setting and tracking\nPerformance improvement plans\nCompliance \u0026amp; Governance:\nAudit trails\nCompliance reporting\nData retention policies\nAdvanced access controls\n3.3 Path To Production This document outlines the current production architecture and operational status for the InsightHR platform deployment. The platform is fully live in the ap-southeast-1 (Singapore) region.\nPlatform Architecture and Access Public URL:\nhttps://insight-hr.io.vn AWS Region:\nap-southeast-1 (Singapore) Frontend:\nReact application hosted on S3 (insighthr-web-app-sg) with CloudFront HTTPS distribution. Backend:\n8 Lambda function groups accessed via API Gateway REST API. Database:\nDynamoDB tables configured with On-Demand capacity.\n6 tables for each team\nEmployee information table\nHistory score table\nAbsent table\nAccount managing tables\nAuthentication:\nCognito User Pool. Live Production Features The following core features have been successfully deployed and are operational:\nAuthentication:\nFull support for email/password login, password reset workflows. User Management:\nComplete CRUD functionality, including bulk import and role-based access. Employee Management:\nFull support for 300+ employees and bulk operations. Performance Score Management:\nManagement of 900+ quarterly scores and calendar-based viewing. Attendance Management:\nProcessing of 9,300+ records, including check-in/check-out kiosk functionality and auto-absence marking. Performance Dashboard:\nLive charts, trend analysis, live clock, and CSV export capabilities. Deployment and Verification Process The standard, repeatable deployment workflow ensures rapid and verifiable updates to the production site:\nBuild:\nnpm run build creates the optimized production asset bundle. Test:\nnpm run preview validates the built bundle locally prior to deployment. Deploy:\naws s3 sync dist/ s3://insighthr-web-app-sg \u0026ndash;region ap-southeast-1 pushes assets to the S3 bucket. Invalidate:\naws cloudfront create-invalidation \u0026ndash;distribution-id E3MHW5VALWTOCI \u0026ndash;paths \u0026ldquo;/*\u0026rdquo; clears the CloudFront CDN cache. Verify:\nFull feature testing is performed on the live public URL. Remaining Production Enhancements The platform is in the final phases of enhancement before full stabilization, with key items planned or in progress:\nPage Integration (In Progress)\nConsolidate all administrative page navigation.\nVerify all features are accessible from the main menu.\nTest role-based routing across all pages.\nFix any integration bugs.\nPolish and Final Deployment (Planned)\nImplement comprehensive error handling and input validation.\nRefine responsive design for full mobile compatibility.\nConduct dedicated Security testing (penetration testing, vulnerability scanning).\nExecute Load testing for scalability validation.\nDevelop user documentation and training materials.\nPerform final production hardening procedures.\nMonitoring and Scalability Strategy\nActive Monitoring:\nCloudWatch Logs are enabled for all Lambda functions and API Gateway endpoints, along with CloudWatch Metrics for performance tracking.\nPlanned Alarms:\nCloudWatch Alarms and SNS notifications are planned for critical error rates and latency.\nScalability:\nAchieved via Serverless Architecture (DynamoDB On-Demand, Lambda, CloudFront CDN).\nDisaster Recovery:\nDynamoDB Point-in-Time Recovery and S3 Versioning are planned to be enabled for critical data/assets. Lambda code is stored in version control for rapid redeployment.\n4 EXPECTED AWS COST BREAKDOWN BY SERVICES AWS Monthly Calculator\nAWS Service Monthly Estimated Cost (USD) AWS Lambda $3.75 Amazon Simple Email Service (SES) $2.25 Amazon DynamoDB $1.52 Amazon Simple Storage Service $0.46 Amazon CloudWatch $0.80 Amazon API Gateway $0.06 Amazon CloudFront $0.00 Amazon Cognito $0.00 Amazon EventBridge $0.00 Amazon IAM $1.60 Amazon KMS $1.03 TOTAL MONTHLY COST $11.53 TOTAL YEARLY COST $138.36 5 TEAM Name Task Role Email / Contact Info Bùi Tấn Phát Dashboard, Manage Employee, Support, Content check Leader btfat3103@gmail.com Nguyễn Ngọc Long CRUD, Config Network / API Gateway, Test function, Slide Member nguyenngoclong216@gmail.com Đặng Nguyễn Minh Duy Database, CloudWatch / CloudLogs, Paper, Slide Member dangnguyenminhduy11b08@gmail.com Đỗ Đăng Khoa Log In/ Registration / Forget Password, UI / UX - Static Web, Paper Member khoado7577@gmail.com Nguyễn Huỳnh Thiên Quang Auto Scoring, Notification System, Slide Member quangkootenhatvutru@gmail.com 6 RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Full-Stack Developers [2] React frontend, Python Lambda backend, API integration $66 Cloud Engineers [3] AWS infrastructure setup, deployment automation, monitoring $66 Other (Please specify) Estimated platform consumption (Lambda, DynamoDB). Paper and present material $0.01 NOTE: Project Phase durations overlap each other.\nProject Phase Duration Man-Days Other (Please specify) Estimated Cost Phase 1: Foundation \u0026amp; Scoring Model 8 Weeks 80 - $42,246.40 (80 x $528.08) Phase 2: Project Setup \u0026amp; Dashboard 2 Weeks 40 - $21,123.20 (40 x $528.08) Phase 3: Absence Mgmt \u0026amp; Notification 1 Week 15 - $7,921.20 (15 x $528.08) Phase 4: Integration, Testing \u0026amp; Handover 1 Week 15 - $7,921.20 (15 x $528.08) Total Hours 1200 Hours 150 Man-Days Total Cost 12 Weeks $79,212.00 Cost Contribution distribution between Partner, Customer, AWS.\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 200 100 7 ACCEPTANCE Project Acceptance Criteria The InsightHR platform will be considered complete and accepted when the following criteria are met.\nCompleted Deliverables\nAll major features implemented and deployed to production\nAuthentication system with Google OAuth\nUser and employee management with bulk operations\nPerformance score management with calendar view\nAttendance system with auto-absence marking\nInteractive dashboard with live clock\nKey Metrics Achieved\n300+ user accounts\n300 employee records across 5 departments\n900+ performance scores tracked\n9,300+ attendance records\nAWS monthly cost: ~$11.53\nSystem uptime: 99.9%+\nZero critical security vulnerabilities\nAcceptance Status\nCurrent Status: Application deployed in cloudfront Production URL: https://d2z6tht6rq32uy.cloudfront.net\nNext Steps\nMinor bug fixing and feature updates\nConduct user acceptance testing\nProvide knowledge transfer and training\n"},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Strengthen understanding of API Gateway fundamentals and integrations Build and deploy Lambda functions for uploading Excel data into DynamoDB Implement full CRUD operations using Lambda and DynamoDB Configure API Gateway to test Lambda functions using Postman Learn and apply request body validation using API Gateway Models Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about API gateway 10/11/2025 10/11/2025 https://youtu.be/YjOjDtprDSo?si=NYb88SlpO2VMhLYx 3 - Create Lambda function to upload excel data to store in dynamoDB 11/11/2025 11/11/2025 4 - Create Lambda function to CRUD data 12/11/2025 12/11/2025 5 - Config API gateway to test Lambda function in Postman 13/11/2025 13/11/2025 https://000079.awsstudygroup.com/ 6 - Body Validation using API Gateway Models 14/11/2025 14/11/2025 https://youtu.be/tmhZbcqlEiQ?si=MBkltclc2rWTlHKr Week 10 Achievements: Completed learning of API Gateway fundamentals and architecture flow Built Lambda function to read Excel files and successfully store parsed data in DynamoDB Developed full CRUD functionality using Lambda + DynamoDB Configured API Gateway and successfully tested endpoints using Postman Implemented body schema validation using API Gateway Models to ensure structured and accurate API requests "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Strengthen understanding of DevOps practices on AWS Infrastructure as Code (IaC) Container services and orchestration Monitoring \u0026amp; Observability tools Learn how to deploy APIs across multiple environments using Stage Variables Understand and implement Canary Deployment on API Gateway Use ANY method on API Gateway for flexible endpoint handling Study Route 53 and understand DNS routing on AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about DevOps on AWS Services + IaC + Container Services Monitoring \u0026amp; Observability 17/11/2025 17/11/2025 3 - Using Stage Variable to deploy API on multi environment 18/11/2025 18/11/2025 https://youtu.be/nubjfS50wFg?si=XQsWE01pyAtyrJh8 4 - Using Canary Deployment on AWS API gateway 19/11/2025 19/11/2025 https://youtu.be/BAjj_XUXnVA?si=21aaQMnOoLTGAeGk 5 - Using Any Method on AWS API gateway 20/11/2025 20/11/2025 https://youtu.be/nXqXJPepMJU?si=mwUbp48qdAHSHaT7 6 - Learn about Route 53 21/11/2025 21/11/2025 https://youtu.be/JRZiQFVWpi8?si=hIE5i0OnqhTw-5nI Week 11 Achievements: Gained knowledge of DevOps concepts within AWS, including IaC, container services, and monitoring tools Successfully deployed APIs across different environments using Stage Variables Implemented Canary Deployment on API Gateway to test changes safely Applied ANY Method in API Gateway for dynamic and flexible routing Studied and understood key routing concepts with Amazon Route 53 "},{"uri":"https://ngocLong216.github.io/fcj_report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Learn CloudFront concepts and understand its pricing model Gain foundational knowledge of Docker and containerization workflow Learn how to use Amazon ECR and push Docker images to ECR Build an Auto Scoring Lambda function to process input data using machine learning models Add functionality to store Auto Scoring results into DynamoDB Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about CloudFront \u0026amp; it pricing - Generate KMS key to encrypt and decrypt dynamoDB data 24/11/2025 24/11/2025 3 - Learn how to use Docker - Config SES to mail absent employee function 25/11/2025 25/11/2025 4 - Learn about ECR \u0026amp; command to push Docker image to ECR Create Rules in EventBridge to trigger lambda every day 26/11/2025 26/11/2025 5 - Create Auto Scoring Function on lambda 27/11/2025 27/11/2025 6 - Add function to store results from Auto Scoring into DynamoDB 28/11/2025 28/11/2025 Week 12 Achievements: Gained understanding of CloudFront architecture and pricing structure Learned how to use Docker, including building and running container images Successfully pushed Docker images to Amazon ECR using AWS CLI commands Implemented Auto Scoring Lambda function capable of processing ML model predictions Added DynamoDB integration to store and retrieve Auto Scoring results "},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.3-architecture/","title":"Project Architecture","tags":[],"description":"","content":"InsightHR Architecture Deep Dive This section provides a detailed look at the InsightHR platform architecture, explaining how different AWS services work together to create a scalable, secure, and cost-effective serverless application.\nHigh-Level Architecture ┌─────────────────────────────────────────────────────────────────┐ │ User Browser │ │ (React + TypeScript) │ └────────────────────────┬────────────────────────────────────────┘ │ HTTPS ▼ ┌─────────────────────────────────────────────────────────────────┐ │ CloudFront CDN │ │ - Global edge locations │ │ - SSL/TLS termination │ │ - Caching static assets │ └────────────────────────┬────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ S3 Static Website │ │ - React SPA hosting │ │ - Static assets (JS, CSS, images) │ └─────────────────────────────────────────────────────────────────┘ │ REST API calls ▼ ┌─────────────────────────────────────────────────────────────────┐ │ API Gateway (REST) │ │ - Request routing │ │ - Cognito authorization │ │ - Request/response transformation │ └────────────────────────┬────────────────────────────────────────┘ │ ┌───────────────┼───────────────┬───────────────┐ ▼ ▼ ▼ ▼ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Lambda │ │ Lambda │ │ Lambda │ │ Lambda │ │ Auth │ │ Employees │ │ Performance │ │ Chatbot │ │ │ │ │ │ Scores │ │ │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ │ │ ▼ ▼ ▼ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ DynamoDB │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ │ │ Users │ │Employees │ │ Scores │ │Attendance│ │ │ └──────────┘ └──────────┘ └──────────┘ └──────────┘ │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ AWS Bedrock │ │ (Claude 3 Haiku Model) │ │ - Natural language processing │ │ - Context-aware responses │ └─────────────────────────────────────────────────────────────────┘ Component Breakdown 1. Frontend Layer Amazon S3 + CloudFront\nS3 Bucket: Hosts the React SPA as static files\nConfigured for static website hosting Stores HTML, JavaScript, CSS, and image assets Versioning enabled for rollback capability CloudFront Distribution: Global CDN for fast content delivery\nEdge locations worldwide for low latency SSL/TLS certificate from ACM Custom domain support (insight-hr.io.vn) Caching policies for optimal performance Origin Access Identity (OAI) for S3 security React Application\nSingle Page Application (SPA) architecture Client-side routing with React Router State management with Zustand TypeScript for type safety Tailwind CSS for styling 2. API Layer Amazon API Gateway (REST)\nEndpoints: RESTful API design\n/auth/* - Authentication endpoints /employees/* - Employee management /performance-scores/* - Performance tracking /attendance/* - Attendance management /chatbot/* - AI chatbot queries Features:\nRequest validation CORS configuration Rate limiting and throttling Request/response transformation API keys and usage plans Authorization: Cognito User Pool Authorizer\nJWT token validation Role-based access control Automatic token refresh 3. Compute Layer AWS Lambda Functions\nAuthentication Service (auth-*-handler)\nLogin with Cognito User registration Google OAuth integration Password reset workflow Token management Employee Service (employees-*-handler)\nCRUD operations for employees Department and position filtering Bulk import from CSV Search functionality Performance Scores Service (performance-scores-handler)\nScore calculation and management Quarterly performance tracking KPI aggregation Role-based data access Chatbot Service (chatbot-handler)\nNatural language query processing Context building from DynamoDB Bedrock API integration Response formatting Attendance Service (attendance-handler)\nCheck-in/check-out operations Attendance history tracking Status management Bulk operations Lambda Configuration:\nRuntime: Python 3.11 Memory: 256-512 MB Timeout: 30-60 seconds Environment variables for configuration IAM roles with least privilege 4. Data Layer Amazon DynamoDB\nTables Structure:\ninsighthr-users-dev\nPrimary Key: userId GSI: email-index Purpose: User authentication and profiles Attributes: email, name, role, employeeId, department insighthr-employees-dev\nPrimary Key: employeeId GSI: department-index Purpose: Employee master data Attributes: name, email, department, position, status insighthr-performance-scores-dev\nPrimary Key: employeeId Sort Key: period GSI: department-period-index Purpose: Quarterly performance scores Attributes: overallScore, kpiScores, calculatedAt insighthr-attendance-history-dev\nPrimary Key: employeeId Sort Key: date GSI: date-index, department-date-index Purpose: Check-in/check-out history Attributes: checkInTime, checkOutTime, status DynamoDB Features:\nOn-demand billing mode Point-in-time recovery Encryption at rest Global secondary indexes for efficient queries TTL for automatic data expiration (if needed) 5. Authentication Layer Amazon Cognito User Pools\nUser Management:\nEmail/password authentication Google OAuth integration User attributes (name, role, department) Password policies and MFA support Token Management:\nJWT tokens (ID, Access, Refresh) Token expiration and refresh Custom claims for roles Security Features:\nPassword complexity requirements Account recovery workflows User verification Brute force protection 6. AI/ML Layer Amazon Bedrock (Claude 3 Haiku)\nCapabilities:\nNatural language understanding Context-aware responses Data querying and analysis Conversational interface Integration:\nInvoked from Lambda function Context built from DynamoDB data Role-based data filtering Response formatting and validation Cost Optimization:\nHaiku model for cost-effectiveness Efficient prompt engineering Response caching where applicable 7. Monitoring Layer Amazon CloudWatch\nLogs:\nLambda function logs API Gateway access logs Error tracking and debugging Metrics:\nAPI request counts Lambda invocations and duration DynamoDB read/write capacity Error rates and latency Alarms:\nHigh error rate alerts Performance degradation Cost threshold warnings CloudWatch Synthetics Canaries:\nLogin flow testing Dashboard availability Chatbot functionality Performance score calculations Data Flow Examples User Login Flow 1. User enters credentials in React app 2. React app calls API Gateway /auth/login 3. API Gateway routes to auth-login-handler Lambda 4. Lambda authenticates with Cognito 5. Cognito returns JWT tokens 6. Lambda stores user session in DynamoDB 7. Tokens returned to React app 8. React app stores tokens in localStorage 9. Subsequent requests include JWT in Authorization header Employee Query Flow 1. User requests employee list in React app 2. React app calls API Gateway /employees with filters 3. API Gateway validates JWT token with Cognito 4. Request routed to employees-handler Lambda 5. Lambda queries DynamoDB employees table 6. Results filtered based on user role 7. Data returned to React app 8. React app displays employees in table Chatbot Query Flow 1. User types question in chatbot interface 2. React app calls API Gateway /chatbot/query 3. API Gateway validates JWT and routes to chatbot-handler 4. Lambda retrieves relevant data from DynamoDB 5. Lambda builds context and calls Bedrock API 6. Bedrock processes query with Claude 3 Haiku 7. Response formatted and returned to React app 8. React app displays answer in chat interface Security Architecture Defense in Depth:\nNetwork Security:\nHTTPS only (enforced by CloudFront) API Gateway with AWS WAF (optional) VPC endpoints for private connectivity (optional) Authentication \u0026amp; Authorization:\nCognito for user authentication JWT tokens for API authorization Role-based access control (RBAC) Least privilege IAM roles Data Security:\nEncryption at rest (DynamoDB, S3) Encryption in transit (TLS 1.2+) Secure credential management No hardcoded secrets Application Security:\nInput validation SQL injection prevention (NoSQL) XSS protection CORS configuration Scalability \u0026amp; Performance Auto-Scaling:\nLambda: Automatic concurrent execution scaling DynamoDB: On-demand capacity mode CloudFront: Global edge network API Gateway: Automatic request handling Performance Optimization:\nCloudFront caching for static assets DynamoDB GSIs for efficient queries Lambda function optimization API response caching High Availability:\nMulti-AZ deployment (automatic) CloudFront global distribution DynamoDB replication Lambda fault tolerance Cost Optimization Strategies:\nOn-demand pricing for variable workloads Lambda free tier utilization CloudFront caching to reduce origin requests DynamoDB query optimization Bedrock Haiku model for cost-effectiveness Estimated Monthly Costs (Development):\nDynamoDB: $0.50 Lambda: Free tier S3 + CloudFront: $1-2 API Gateway: $0.10 Bedrock: $0.0004 per query Total: $2-5/month Next Steps Now that you understand the architecture, let\u0026rsquo;s proceed to Setup AWS Environment to start building the platform.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs I have translated.:\nBlog 1 - Clario enhances the quality of the clinical trial documentation process with Amazon Bedrock This blog introduces how Clario, a global leader in endpoint data solutions for clinical trials, is enhancing the quality and efficiency of its documentation process using Amazon Bedrock. By modernizing the creation of clinical trial documents through generative AI, Clario has streamlined operations, reduced manual errors, and improved consistency across critical study materials. The collaboration between Clario and AWS demonstrates how generative AI can accelerate document generation workflows, strengthen data integrity, and support faster delivery of innovative therapies to patients worldwide.\nBlog 2 - Cleeng’s ChurnIQ AI-ssistant: Subscriber Analytics Powered by Amazon Bedrock This blog introduces how Cleeng leveraged Amazon Bedrock to power ChurnIQ AI-ssistant, a generative AI–driven analytics solution that enables digital media companies to better understand and retain subscribers. Built on AWS, ChurnIQ AI-ssistant allows teams to query complex subscriber data in natural language, visualize insights instantly, and take proactive actions to reduce churn. The solution exemplifies how generative AI can democratize access to data analytics, enhance decision-making, and drive measurable improvements in subscriber engagement and lifetime value.\nBlog 3 - Cloud cost savings: 10 tips for academic institutions This blog introduces ten practical strategies to help academic institutions optimize their cloud spending on Amazon Web Services (AWS) while maintaining performance and scalability. As universities and research organizations increasingly adopt cloud infrastructure, cost management has become a key priority. The guidance provided in this post highlights AWS tools, services, and best practices that enable institutions to monitor usage, automate savings, and achieve financial efficiency without compromising on innovation or agility.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.4-setup-aws/","title":"Setup AWS Environment","tags":[],"description":"","content":"Setting Up AWS Environment This section guides you through configuring your AWS environment for the InsightHR platform, including IAM roles, policies, and initial service configuration.\nOverview We\u0026rsquo;ll set up:\nIAM roles for Lambda functions IAM policies for service access AWS region configuration Service quotas verification Bedrock model access Step 1: Configure AWS Region Set your default region to Singapore (ap-southeast-1):\n# Configure AWS CLI aws configure set region ap-southeast-1 # Verify configuration aws configure get region Why Singapore?\nAll required services available Good latency for Southeast Asia Bedrock Claude 3 Haiku available Step 2: Create IAM Role for Lambda Lambda functions need an execution role to access AWS services.\nCreate Trust Policy lambda-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the Role # Create IAM role aws iam create-role \\ --role-name insighthr-lambda-role \\ --assume-role-policy-document file://lambda-trust-policy.json \\ --description \u0026#34;Execution role for InsightHR Lambda functions\u0026#34; Step 3: Create IAM Policies DynamoDB Access Policy dynamodb-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:ap-southeast-1:*:table/insighthr-*\u0026#34; ] } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-dynamodb-policy \\ --policy-document file://dynamodb-policy.json Cognito Access Policy cognito-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:cognito-idp:ap-southeast-1:*:userpool/*\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-cognito-policy \\ --policy-document file://cognito-policy.json Bedrock Access Policy bedrock-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-bedrock-policy \\ --policy-document file://bedrock-policy.json Step 4: Attach Policies to Role # Get AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Attach AWS managed policy for Lambda basic execution aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # Attach custom policies aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-dynamodb-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-cognito-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-bedrock-policy Step 5: Create IAM Role for API Gateway API Gateway needs a role to write logs to CloudWatch.\napi-gateway-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the role:\naws iam create-role \\ --role-name insighthr-apigateway-role \\ --assume-role-policy-document file://api-gateway-trust-policy.json # Attach CloudWatch logs policy aws iam attach-role-policy \\ --role-name insighthr-apigateway-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs Step 6: Enable Bedrock Model Access Request access to Claude 3 Haiku model:\nUsing AWS Console:\nNavigate to Amazon Bedrock Click \u0026ldquo;Model access\u0026rdquo; in left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box Click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant) Verify Access:\naws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude-3-haiku`)].modelId\u0026#39; Step 7: Verify Service Quotas Check service limits for your account:\n# Lambda concurrent executions aws service-quotas get-service-quota \\ --service-code lambda \\ --quota-code L-B99A9384 \\ --region ap-southeast-1 # DynamoDB tables aws service-quotas get-service-quota \\ --service-code dynamodb \\ --quota-code L-F98FE922 \\ --region ap-southeast-1 # API Gateway requests per second aws service-quotas get-service-quota \\ --service-code apigateway \\ --quota-code L-8A5B8E43 \\ --region ap-southeast-1 Step 8: Create S3 Bucket for Deployment Artifacts # Create bucket for Lambda deployment packages aws s3 mb s3://insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --region ap-southeast-1 # Enable versioning aws s3api put-bucket-versioning \\ --bucket insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --versioning-configuration Status=Enabled Step 9: Set Up CloudWatch Log Groups Pre-create log groups for Lambda functions:\n# Create log groups LOG_GROUPS=( \u0026#34;/aws/lambda/insighthr-auth-login-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-register-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-google-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-employees-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-performance-scores-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-chatbot-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-attendance-handler\u0026#34; ) for log_group in \u0026#34;${LOG_GROUPS[@]}\u0026#34;; do aws logs create-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 # Set retention to 7 days aws logs put-retention-policy \\ --log-group-name $log_group \\ --retention-in-days 7 \\ --region ap-southeast-1 done Step 10: Configure Environment Variables Create a configuration file for environment variables:\nconfig.env:\n# AWS Configuration export AWS_REGION=ap-southeast-1 export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # DynamoDB Tables export USERS_TABLE=insighthr-users-dev export EMPLOYEES_TABLE=insighthr-employees-dev export PERFORMANCE_SCORES_TABLE=insighthr-performance-scores-dev export ATTENDANCE_HISTORY_TABLE=insighthr-attendance-history-dev # Cognito (will be set after Cognito setup) export USER_POOL_ID= export CLIENT_ID= # Bedrock export BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0 export BEDROCK_REGION=ap-southeast-1 # Lambda Role ARN export LAMBDA_ROLE_ARN=arn:aws:iam::${AWS_ACCOUNT_ID}:role/insighthr-lambda-role Load the configuration:\nsource config.env Verification Checklist Verify your AWS environment is ready:\n# Check IAM role exists aws iam get-role --role-name insighthr-lambda-role # Check policies are attached aws iam list-attached-role-policies --role-name insighthr-lambda-role # Check Bedrock access aws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude`)].modelId\u0026#39; # Check S3 bucket exists aws s3 ls s3://insighthr-deployment-artifacts-${ACCOUNT_ID} # Check CloudWatch log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 IAM Setup Summary Resource Purpose Policies Attached insighthr-lambda-role Lambda execution AWSLambdaBasicExecutionRole, DynamoDB, Cognito, Bedrock insighthr-apigateway-role API Gateway logging AmazonAPIGatewayPushToCloudWatchLogs insighthr-dynamodb-policy DynamoDB access Custom policy for table operations insighthr-cognito-policy Cognito access Custom policy for user management insighthr-bedrock-policy Bedrock access Custom policy for model invocation Security Best Practices ✅ Least Privilege: Policies grant only necessary permissions ✅ Resource Restrictions: Policies limited to insighthr-* resources ✅ Separate Roles: Different roles for different services ✅ CloudWatch Logging: All Lambda functions log to CloudWatch ✅ Versioning: S3 bucket versioning enabled for artifacts\nTroubleshooting IAM Role Creation Fails:\nCheck IAM permissions Verify trust policy syntax Ensure role name is unique Policy Attachment Fails:\nVerify policy ARN is correct Check role exists Ensure you have iam:AttachRolePolicy permission Bedrock Access Denied:\nRequest model access in Bedrock console Wait for approval (usually instant for Haiku) Verify region supports Bedrock Service Quota Issues:\nRequest quota increase in Service Quotas console Use different region if needed Contact AWS support for urgent increases Next Steps With the AWS environment configured, proceed to Database Setup to create DynamoDB tables.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: ​AI/ML/GenAI on AWS\nDate \u0026amp; Time: 15/11/2025 • 08:00 – 11:30\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: ​DevOps on AWS\nDate \u0026amp; Time: 17/11/2025 • 08:30 – 17:00\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.5-database-setup/","title":"Database Setup (DynamoDB)","tags":[],"description":"","content":"DynamoDB Database Setup In this section, you\u0026rsquo;ll create and configure the DynamoDB tables required for the InsightHR platform.\nOverview InsightHR uses Amazon DynamoDB, a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. We\u0026rsquo;ll create 4 main tables:\nUsers Table - Authentication and user profiles Employees Table - Employee master data Performance Scores Table - Quarterly performance tracking Attendance History Table - Check-in/check-out records Table Design Principles DynamoDB Best Practices:\nUse partition keys for even data distribution Add sort keys for range queries Create GSIs for alternate query patterns Denormalize data for read performance Use on-demand billing for variable workloads Step 1: Create Users Table Table Configuration:\nTable Name: insighthr-users-dev Partition Key: userId (String) Billing Mode: On-demand Using AWS Console:\nNavigate to DynamoDB in AWS Console Click \u0026ldquo;Create table\u0026rdquo; Enter table name: insighthr-users-dev Partition key: userId (String) Table settings: Use default settings Billing mode: On-demand Click \u0026ldquo;Create table\u0026rdquo; Add Global Secondary Index:\nSelect the table Go to \u0026ldquo;Indexes\u0026rdquo; tab Click \u0026ldquo;Create index\u0026rdquo; Index name: email-index Partition key: email (String) Projected attributes: All Click \u0026ldquo;Create index\u0026rdquo; Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=userId,AttributeType=S \\ AttributeName=email,AttributeType=S \\ --key-schema \\ AttributeName=userId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI aws dynamodb update-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=email,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;email-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;email\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;userId\u0026#34;: \u0026#34;user-123\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Admin User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 2: Create Employees Table Table Configuration:\nTable Name: insighthr-employees-dev Partition Key: employeeId (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department queries aws dynamodb update-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;hireDate\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;managerId\u0026#34;: \u0026#34;DEV-000\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 3: Create Performance Scores Table Table Configuration:\nTable Name: insighthr-performance-scores-dev Partition Key: employeeId (String) Sort Key: period (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=period,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=period,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department-period queries aws dynamodb update-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=period,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-period-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;period\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;scoreId\u0026#34;: \u0026#34;score-123\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;period\u0026#34;: \u0026#34;2025-Q1\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;overallScore\u0026#34;: 85.5, \u0026#34;kpiScores\u0026#34;: { \u0026#34;KPI\u0026#34;: 85.0, \u0026#34;completed_task\u0026#34;: 88.0, \u0026#34;feedback_360\u0026#34;: 83.5 }, \u0026#34;calculatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 4: Create Attendance History Table Table Configuration:\nTable Name: insighthr-attendance-history-dev Partition Key: employeeId (String) Sort Key: date (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=date,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=date,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 # Create GSI for department-date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-12-09\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;checkInTime\u0026#34;: \u0026#34;09:00:00\u0026#34;, \u0026#34;checkOutTime\u0026#34;: \u0026#34;18:00:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;present\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;On time\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T09:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T18:00:00Z\u0026#34; } Step 5: Verify Table Creation Using AWS Console:\nNavigate to DynamoDB Check that all 4 tables are listed Verify each table status is \u0026ldquo;Active\u0026rdquo; Check GSIs are created and active Using AWS CLI:\n# List all tables aws dynamodb list-tables --region ap-southeast-1 # Describe specific table aws dynamodb describe-table \\ --table-name insighthr-users-dev \\ --region ap-southeast-1 Step 6: Load Sample Data (Optional) For testing purposes, you can load sample data into the tables.\nCreate sample data file (sample-users.json):\n{ \u0026#34;insighthr-users-dev\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;userId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user-001\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;admin@example.com\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin User\u0026#34;}, \u0026#34;role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}, \u0026#34;employeeId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV-001\u0026#34;}, \u0026#34;department\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV\u0026#34;}, \u0026#34;isActive\u0026#34;: {\u0026#34;BOOL\u0026#34;: true}, \u0026#34;createdAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;}, \u0026#34;updatedAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;} } } } ] } Load data:\naws dynamodb batch-write-item \\ --request-items file://sample-users.json \\ --region ap-southeast-1 Database Configuration Summary Table Partition Key Sort Key GSIs Purpose insighthr-users-dev userId - email-index User authentication insighthr-employees-dev employeeId - department-index Employee data insighthr-performance-scores-dev employeeId period department-period-index Performance tracking insighthr-attendance-history-dev employeeId date date-index, department-date-index Attendance records Best Practices Implemented ✅ Partition Key Design: Even distribution of data ✅ Sort Keys: Enable range queries for time-series data ✅ GSIs: Support alternate query patterns ✅ On-Demand Billing: Cost-effective for variable workloads ✅ Naming Convention: Consistent table naming with environment suffix\nTroubleshooting Table Creation Fails:\nCheck IAM permissions for DynamoDB Verify region is correct Ensure table name doesn\u0026rsquo;t already exist GSI Creation Fails:\nWait for table to be ACTIVE before creating GSI Check attribute definitions match Verify IAM permissions High Costs:\nUse on-demand billing for development Monitor read/write capacity units Optimize query patterns Next Steps With the database tables created, proceed to Authentication Service to set up Cognito and authentication Lambda functions.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"InsightHR - Serverless HR Automation Platform Workshop Overview InsightHR is a modern, fully serverless HR automation platform built on AWS that demonstrates best practices for cloud-native application development. This workshop guides you through building and deploying a complete production-ready application using AWS services.\nWhat You\u0026rsquo;ll Build A comprehensive HR management system featuring:\nEmployee Management: Full CRUD operations with advanced filtering Performance Tracking: Quarterly performance scores and KPI management Attendance System: Real-time check-in/check-out with history tracking AI Chatbot: Natural language queries powered by AWS Bedrock (Claude 3) Dashboard Analytics: Interactive performance visualization Role-Based Access Control: Admin, Manager, and Employee roles Authentication: Email/password and Google OAuth via AWS Cognito Architecture Highlights ✅ 100% Serverless - No EC2 instances to manage ✅ Scalable - Auto-scales with demand ✅ Cost-Effective - Pay only for what you use ✅ Secure - Built-in security with Cognito and IAM ✅ Modern Stack - React + TypeScript + Python ✅ Production-Ready - CloudWatch monitoring and custom domain AWS Services Used Frontend: S3 + CloudFront + Route53 Backend: Lambda + API Gateway + DynamoDB Authentication: Cognito User Pools AI/ML: Amazon Bedrock (Claude 3 Haiku) Monitoring: CloudWatch + Synthetics Canaries Security: IAM + ACM (SSL Certificates) Workshop Content Workshop Overview Prerequisites Project Architecture Setup AWS Environment Database Setup (DynamoDB) Authentication Service Backend Services Frontend Development Deployment Testing \u0026amp; Monitoring Cleanup Learning Outcomes By completing this workshop, you will learn:\nHow to design and implement serverless architectures Best practices for AWS Lambda and API Gateway DynamoDB data modeling and optimization AWS Cognito authentication flows Integration with AWS Bedrock for AI capabilities CloudFront CDN configuration Infrastructure as Code principles Production deployment strategies Cost optimization techniques Prerequisites AWS Account with appropriate permissions Basic knowledge of JavaScript/TypeScript and Python Familiarity with React framework Understanding of REST APIs AWS CLI installed and configured Estimated Time Full Workshop: 4-6 hours Core Features Only: 2-3 hours Cost Estimate Running this workshop will incur minimal AWS costs:\nDynamoDB: ~$0.50/month (on-demand pricing) Lambda: Free tier covers most usage S3 + CloudFront: ~$1-2/month API Gateway: ~$0.10/month Bedrock: ~$0.0004 per query Total: ~$2-5/month for development Remember to clean up resources after completing the workshop to avoid ongoing charges.\nSupport For questions or issues during the workshop:\nCheck the troubleshooting sections in each module Review AWS documentation links provided Refer to the GitHub repository for code samples Let\u0026rsquo;s get started! 🚀\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.6-authentication/","title":"Authentication Service","tags":[],"description":"","content":"Authentication Service Setup This section covers setting up AWS Cognito for user authentication and implementing authentication Lambda functions.\nOverview The authentication system includes:\nAWS Cognito User Pool - User management and authentication Login Handler - Email/password authentication Registration Handler - New user signup Google OAuth Handler - Social login integration Password Reset - Password recovery workflow Step 1: Create Cognito User Pool Using AWS Console Navigate to Amazon Cognito\nClick \u0026ldquo;Create user pool\u0026rdquo;\nConfigure sign-in experience:\nSign-in options: Email User name requirements: Allow email addresses Click \u0026ldquo;Next\u0026rdquo; Configure security requirements:\nPassword policy: Cognito defaults Multi-factor authentication: Optional User account recovery: Email only Click \u0026ldquo;Next\u0026rdquo; Configure sign-up experience:\nSelf-registration: Enabled Required attributes: name, email Click \u0026ldquo;Next\u0026rdquo; Configure message delivery:\nEmail provider: Send email with Cognito FROM email address: no-reply@verificationemail.com Click \u0026ldquo;Next\u0026rdquo; Integrate your app:\nUser pool name: insighthr-user-pool App client name: insighthr-web-client Client secret: Don\u0026rsquo;t generate Authentication flows: ALLOW_USER_PASSWORD_AUTH, ALLOW_REFRESH_TOKEN_AUTH Click \u0026ldquo;Next\u0026rdquo; Review and create\nUsing AWS CLI # Create user pool aws cognito-idp create-user-pool \\ --pool-name insighthr-user-pool \\ --policies \u0026#39;{ \u0026#34;PasswordPolicy\u0026#34;: { \u0026#34;MinimumLength\u0026#34;: 8, \u0026#34;RequireUppercase\u0026#34;: true, \u0026#34;RequireLowercase\u0026#34;: true, \u0026#34;RequireNumbers\u0026#34;: true, \u0026#34;RequireSymbols\u0026#34;: false } }\u0026#39; \\ --auto-verified-attributes email \\ --username-attributes email \\ --schema \u0026#39;[ { \u0026#34;Name\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true }, { \u0026#34;Name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true } ]\u0026#39; \\ --region ap-southeast-1 # Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;User Pool ID: $USER_POOL_ID\u0026#34; Create App Client # Create app client aws cognito-idp create-user-pool-client \\ --user-pool-id $USER_POOL_ID \\ --client-name insighthr-web-client \\ --no-generate-secret \\ --explicit-auth-flows ALLOW_USER_PASSWORD_AUTH ALLOW_REFRESH_TOKEN_AUTH ALLOW_USER_SRP_AUTH \\ --region ap-southeast-1 # Get client ID CLIENT_ID=$(aws cognito-idp list-user-pool-clients \\ --user-pool-id $USER_POOL_ID \\ --query \u0026#34;UserPoolClients[?ClientName==\u0026#39;insighthr-web-client\u0026#39;].ClientId\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;Client ID: $CLIENT_ID\u0026#34; Step 2: Configure Cognito for Development For development, disable email verification requirement:\n# Update user pool to auto-verify emails aws cognito-idp update-user-pool \\ --user-pool-id $USER_POOL_ID \\ --auto-verified-attributes email \\ --region ap-southeast-1 Step 3: Create Login Lambda Function Create Function Code auth_login_handler.py:\nimport json import boto3 import os from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] # Authenticate with Cognito response = cognito_client.admin_initiate_auth( UserPoolId=USER_POOL_ID, ClientId=CLIENT_ID, AuthFlow=\u0026#39;ADMIN_NO_SRP_AUTH\u0026#39;, AuthParameters={ \u0026#39;USERNAME\u0026#39;: email, \u0026#39;PASSWORD\u0026#39;: password } ) # Get user attributes user_response = cognito_client.admin_get_user( UserPoolId=USER_POOL_ID, Username=email ) # Extract user info user_attributes = {attr[\u0026#39;Name\u0026#39;]: attr[\u0026#39;Value\u0026#39;] for attr in user_response[\u0026#39;UserAttributes\u0026#39;]} # Get user from DynamoDB table = dynamodb.Table(USERS_TABLE) db_response = table.get_item( Key={\u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;]} ) user_data = db_response.get(\u0026#39;Item\u0026#39;, {}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;tokens\u0026#39;: { \u0026#39;accessToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;AccessToken\u0026#39;], \u0026#39;idToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;IdToken\u0026#39;], \u0026#39;refreshToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;RefreshToken\u0026#39;] }, \u0026#39;user\u0026#39;: { \u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;], \u0026#39;email\u0026#39;: user_attributes[\u0026#39;email\u0026#39;], \u0026#39;name\u0026#39;: user_attributes.get(\u0026#39;name\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;role\u0026#39;: user_data.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;), \u0026#39;department\u0026#39;: user_data.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;employeeId\u0026#39;: user_data.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) } }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;NotAuthorizedException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 401, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Invalid credentials\u0026#39;}) } elif error_code == \u0026#39;UserNotFoundException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User not found\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Create requirements.txt boto3\u0026gt;=1.26.0 Package and Deploy # Create deployment package mkdir -p lambda/auth/package cd lambda/auth pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy Lambda function aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 4: Create Registration Lambda Function auth_register_handler.py:\nimport json import boto3 import os import uuid from datetime import datetime from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] name = body[\u0026#39;name\u0026#39;] role = body.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;) department = body.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;) employee_id = body.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) # Create user in Cognito cognito_response = cognito_client.sign_up( ClientId=CLIENT_ID, Username=email, Password=password, UserAttributes=[ {\u0026#39;Name\u0026#39;: \u0026#39;email\u0026#39;, \u0026#39;Value\u0026#39;: email}, {\u0026#39;Name\u0026#39;: \u0026#39;name\u0026#39;, \u0026#39;Value\u0026#39;: name} ] ) user_id = cognito_response[\u0026#39;UserSub\u0026#39;] # Auto-confirm user for development cognito_client.admin_confirm_sign_up( UserPoolId=USER_POOL_ID, Username=email ) # Store user in DynamoDB table = dynamodb.Table(USERS_TABLE) timestamp = datetime.utcnow().isoformat() + \u0026#39;Z\u0026#39; table.put_item( Item={ \u0026#39;userId\u0026#39;: user_id, \u0026#39;email\u0026#39;: email, \u0026#39;name\u0026#39;: name, \u0026#39;role\u0026#39;: role, \u0026#39;department\u0026#39;: department, \u0026#39;employeeId\u0026#39;: employee_id, \u0026#39;isActive\u0026#39;: True, \u0026#39;createdAt\u0026#39;: timestamp, \u0026#39;updatedAt\u0026#39;: timestamp } ) return { \u0026#39;statusCode\u0026#39;: 201, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;User registered successfully\u0026#39;, \u0026#39;userId\u0026#39;: user_id }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;UsernameExistsException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 409, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User already exists\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Deploy the registration handler:\nzip -g auth-register-handler.zip auth_register_handler.py aws lambda create-function \\ --function-name insighthr-auth-register-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_register_handler.lambda_handler \\ --zip-file fileb://auth-register-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 5: Create API Gateway Endpoints Create REST API # Create API API_ID=$(aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;API ID: $API_ID\u0026#34; Create Cognito Authorizer # Create authorizer AUTHORIZER_ID=$(aws apigateway create-authorizer \\ --rest-api-id $API_ID \\ --name insighthr-cognito-authorizer \\ --type COGNITO_USER_POOLS \\ --provider-arns arn:aws:cognito-idp:ap-southeast-1:${ACCOUNT_ID}:userpool/${USER_POOL_ID} \\ --identity-source method.request.header.Authorization \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;Authorizer ID: $AUTHORIZER_ID\u0026#34; Create /auth Resource # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Create /auth resource AUTH_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part auth \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create /auth/login resource LOGIN_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AUTH_ID \\ --path-part login \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create POST method for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --authorization-type NONE \\ --region ap-southeast-1 # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:${ACCOUNT_ID}:function:insighthr-auth-login-handler/invocations \\ --region ap-southeast-1 # Grant API Gateway permission to invoke Lambda aws lambda add-permission \\ --function-name insighthr-auth-login-handler \\ --statement-id apigateway-invoke \\ --action lambda:InvokeFunction \\ --principal apigateway.amazonaws.com \\ --source-arn \u0026#34;arn:aws:execute-api:ap-southeast-1:${ACCOUNT_ID}:${API_ID}/*/*\u0026#34; \\ --region ap-southeast-1 Step 6: Enable CORS # Enable CORS for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --authorization-type NONE \\ --region ap-southeast-1 aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --type MOCK \\ --request-templates \u0026#39;{\u0026#34;application/json\u0026#34;: \u0026#34;{\\\u0026#34;statusCode\\\u0026#34;: 200}\u0026#34;}\u0026#39; \\ --region ap-southeast-1 aws apigateway put-method-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: true }\u0026#39; \\ --region ap-southeast-1 aws apigateway put-integration-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;POST,OPTIONS\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;*\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34; }\u0026#39; \\ --region ap-southeast-1 Step 7: Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; \\ --region ap-southeast-1 # Get API endpoint echo \u0026#34;API Endpoint: https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 8: Test Authentication Test Registration curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Employee\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34; }\u0026#39; Test Login curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34; }\u0026#39; Authentication Flow Summary 1. User submits credentials → Frontend 2. Frontend calls /auth/login → API Gateway 3. API Gateway routes to Lambda → auth-login-handler 4. Lambda authenticates with Cognito → Cognito User Pool 5. Cognito returns JWT tokens → Lambda 6. Lambda retrieves user data → DynamoDB 7. Lambda returns tokens + user data → Frontend 8. Frontend stores tokens → localStorage 9. Subsequent requests include JWT → Authorization header Security Best Practices ✅ Password Policy: Strong password requirements ✅ JWT Tokens: Short-lived access tokens ✅ Refresh Tokens: Long-lived for token renewal ✅ HTTPS Only: All communication encrypted ✅ CORS: Properly configured for frontend domain ✅ No Secrets: Client doesn\u0026rsquo;t use client secret\nTroubleshooting Cognito Authentication Fails:\nVerify user pool ID and client ID Check password meets requirements Ensure user is confirmed Lambda Permission Denied:\nCheck IAM role has Cognito permissions Verify Lambda execution role attached CORS Errors:\nEnable CORS on API Gateway Check Access-Control-Allow-Origin header Verify OPTIONS method configured Next Steps With authentication configured, proceed to Backend Services to implement employee, performance, and chatbot APIs.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS from 08/09/2025 to 28/11/2025, I had the chance to learn, practice, and apply the knowledge gained in school to a real working environment. I took part in the Insight HR project, where I strengthened my coding, communication, and reporting skills.\nRegarding my work ethic, I consistently aimed to complete my tasks effectively, followed company policies, and proactively collaborated with colleagues to enhance overall productivity.\nTo provide an objective overview of my internship experience, I would like to assess myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Learned how to communicate more effectively in daily interactions and in the workplace, as well as how to handle situations.\nBecame more proactive in my work.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.7-backend-services/","title":"Backend Services","tags":[],"description":"","content":"Backend Services Implementation This section covers implementing the core backend Lambda functions for employee management, performance tracking, attendance, and AI chatbot.\nOverview We\u0026rsquo;ll implement four main services:\nEmployee Service - CRUD operations for employee records Performance Scores Service - Quarterly performance tracking Attendance Service - Check-in/check-out management Chatbot Service - AI-powered natural language queries Employee Service The employee service handles all employee-related operations with department and position filtering.\nKey Features:\nCRUD operations (Create, Read, Update, Delete) Department filtering (DEV, QA, DAT, SEC, AI) Position filtering (Junior, Mid, Senior, Lead, Manager) Status filtering (active, inactive) Search by name or employee ID Bulk import from CSV API Endpoints:\nGET /employees - List all employees GET /employees/{id} - Get employee by ID POST /employees - Create new employee PUT /employees/{id} - Update employee DELETE /employees/{id} - Delete employee POST /employees/bulk - Bulk import from CSV Performance Scores Service Manages quarterly performance scores with automatic calculation.\nKey Features:\nAutomatic score calculation (average of KPI, completed_task, feedback_360) Department and period filtering Role-based data access (Admin: all, Manager: department, Employee: own) Denormalized employee data for performance Bulk import from CSV API Endpoints:\nGET /performance-scores - List all scores GET /performance-scores/{id}/{period} - Get specific score POST /performance-scores - Create score PUT /performance-scores/{id}/{period} - Update score DELETE /performance-scores/{id}/{period} - Delete score POST /performance-scores/bulk - Bulk import Attendance Service Tracks employee attendance with check-in/check-out functionality.\nKey Features:\nReal-time check-in/check-out Date range filtering Department filtering Status tracking (present, absent, late, half-day) Bulk operations API Endpoints:\nGET /attendance - List attendance records GET /attendance/{id}/{date} - Get specific record POST /attendance/check-in - Check in POST /attendance/check-out - Check out PUT /attendance/{id}/{date} - Update record DELETE /attendance/{id}/{date} - Delete record Chatbot Service AI-powered chatbot using AWS Bedrock (Claude 3 Haiku).\nKey Features:\nAWS Bedrock integration (Claude 3 Haiku) Natural language query processing Context building from DynamoDB Role-based data filtering Cost-effective ($0.0004 per query) Supported Queries:\nEmployee information (\u0026ldquo;Who is DEV-001?\u0026rdquo;) Performance queries (\u0026ldquo;What\u0026rsquo;s the average score for Q1 2025?\u0026rdquo;) Department statistics (\u0026ldquo;Compare DEV and QA performance\u0026rdquo;) Trend analysis (\u0026ldquo;Performance trends over quarters\u0026rdquo;) API Endpoint:\nPOST /chatbot/query Implementation Pattern All services follow this pattern:\nRequest Validation - Validate input parameters Authorization - Check user permissions based on role DynamoDB Operations - Query or update data Response Formatting - Return standardized JSON response Error Handling - Catch and return appropriate errors Deployment Deploy all backend services using the Lambda deployment scripts provided in the workshop materials. Each service is packaged with its dependencies and deployed to AWS Lambda with appropriate environment variables.\nTesting Test each service using the provided test scripts or curl commands. Verify:\nCRUD operations work correctly Filtering and search functionality Role-based access control Error handling Performance and response times Next Steps With backend services implemented, proceed to Frontend Development to build the React application.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment at FCJ is very friendly and open. Team members are always willing to help whenever I encounter difficulties, even outside of working hours. The workspace is well-organized and comfortable, helping me stay focused.\n2. Support from Mentor / Team Admin\nMy mentor provided thorough guidance, explained clearly whenever I didn’t fully understand something, and always encouraged me to ask questions. The team admin supported administrative procedures, documents, and created favorable conditions for my work. I especially appreciated that my mentor allowed me to explore solutions on my own instead of giving the answer immediately.\n3. Relevance of Work to Academic Major\nThe assigned tasks matched well with my academic background while also introducing me to new topics I had not encountered before. This helped reinforce my foundational knowledge and build additional hands-on experience.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship, I gained various new skills, including working with cloud tools, collaborating in a team, and communicating professionally in a corporate setting. My mentor also shared practical insights that helped me better shape my future career direction.\n5. Company Culture \u0026amp; Team Spirit\nThe company fosters a positive culture where everyone respects each other and works efficiently while still maintaining a comfortable atmosphere. During urgent projects, the whole team collaborates and supports one another, regardless of role. This made me feel truly integrated into the team, even as an intern.\n6. Internship Policies / Benefits\nThe company offers internship allowances and provides flexibility with working hours, creating supportive conditions for interns.\nSuggestions \u0026amp; Expectations What I appreciate the most is the flexible work schedule, allowing me to freely arrange and register office days.\nIf possible, I would recommend friends in the same field to intern at AWS because, besides the knowledge learned at school, interns also gain valuable experience in cloud computing.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.8-frontend/","title":"Frontend Development","tags":[],"description":"","content":"Frontend Development This section covers building the React frontend application for InsightHR.\nOverview The frontend is a Single Page Application (SPA) built with:\nReact 18 with TypeScript Vite 7.2 for fast builds Tailwind CSS 3.4 for styling (Frutiger Aero theme) Zustand 5.0 for state management React Hook Form + Zod for form validation Recharts 3.4 for data visualization React Router v7 for routing Project Structure insighthr-web/ ├── src/ │ ├── components/ │ │ ├── auth/ # Login, Register, ProtectedRoute │ │ ├── admin/ # User/Employee/Score Management │ │ ├── dashboard/ # Charts, Tables, Filters │ │ ├── attendance/ # Check-in/out, History │ │ ├── chatbot/ # AI Chat Interface │ │ ├── profile/ # User Profile │ │ ├── common/ # Reusable UI components │ │ └── layout/ # Header, Sidebar, Footer │ ├── pages/ # Page components │ ├── services/ # API service layer │ ├── store/ # Zustand state stores │ ├── types/ # TypeScript type definitions │ ├── utils/ # Utility functions │ └── styles/ # Global styles and theme ├── public/ # Static assets └── package.json # Dependencies Key Components Authentication Components LoginForm: Email/password and Google OAuth login RegisterForm: New user registration ProtectedRoute: Route guard for authenticated users ChangePasswordModal: Password change dialog Admin Panel Components UserManagement: User CRUD with role assignment EmployeeManagement: Employee CRUD with filters PerformanceScoreManagement: Score management with bulk import PasswordRequestsPanel: Password reset request handling Dashboard Components PerformanceDashboard: Main analytics dashboard BarChart: Department performance comparison LineChart: Trend analysis over time PieChart: Score distribution DataTable: Sortable, filterable data grid FilterPanel: Date range and department filters ExportButton: CSV/Excel export Chatbot Components MessageList: Conversation history display MessageInput: User input with send button TypingIndicator: Loading animation ChatbotInstructions: Usage guide Attendance Components CheckInCheckOut: Quick check-in/out buttons AttendanceManagement: Full attendance interface AttendanceCalendarView: Calendar visualization AttendanceRecordsList: History table State Management Using Zustand for global state:\n// Auth Store interface AuthState { user: User | null; tokens: Tokens | null; isAuthenticated: boolean; login: (email: string, password: string) =\u0026gt; Promise\u0026lt;void\u0026gt;; logout: () =\u0026gt; void; } // Employee Store interface EmployeeState { employees: Employee[]; loading: boolean; error: string | null; fetchEmployees: (filters?: Filters) =\u0026gt; Promise\u0026lt;void\u0026gt;; createEmployee: (data: EmployeeInput) =\u0026gt; Promise\u0026lt;void\u0026gt;; } API Integration All API calls go through a centralized service layer with axios interceptors for authentication:\nconst api = axios.create({ baseURL: import.meta.env.VITE_API_BASE_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); // Add auth token to all requests api.interceptors.request.use((config) =\u0026gt; { const token = localStorage.getItem(\u0026#39;idToken\u0026#39;); if (token) { config.headers.Authorization = `Bearer ${token}`; } return config; }); Routing React Router v7 handles navigation:\nconst router = createBrowserRouter([ { path: \u0026#39;/login\u0026#39;, element: \u0026lt;LoginPage /\u0026gt; }, { path: \u0026#39;/register\u0026#39;, element: \u0026lt;RegisterForm /\u0026gt; }, { path: \u0026#39;/\u0026#39;, element: \u0026lt;ProtectedRoute\u0026gt;\u0026lt;Layout /\u0026gt;\u0026lt;/ProtectedRoute\u0026gt;, children: [ { path: \u0026#39;/\u0026#39;, element: \u0026lt;DashboardPage /\u0026gt; }, { path: \u0026#39;/admin\u0026#39;, element: \u0026lt;AdminPage /\u0026gt; }, { path: \u0026#39;/employees\u0026#39;, element: \u0026lt;EmployeesPage /\u0026gt; }, { path: \u0026#39;/performance-scores\u0026#39;, element: \u0026lt;PerformanceScoresPage /\u0026gt; }, { path: \u0026#39;/attendance\u0026#39;, element: \u0026lt;AttendancePage /\u0026gt; }, { path: \u0026#39;/chatbot\u0026#39;, element: \u0026lt;ChatbotPage /\u0026gt; }, { path: \u0026#39;/profile\u0026#39;, element: \u0026lt;ProfilePage /\u0026gt; } ] } ]); Environment Configuration Create .env file:\nVITE_API_BASE_URL=https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=your-user-pool-id VITE_COGNITO_CLIENT_ID=your-client-id Development # Install dependencies npm install # Start development server npm run dev # Build for production npm run build # Preview production build npm run preview Styling Tailwind CSS with custom Frutiger Aero theme provides a modern, clean interface with:\nGradient backgrounds Glassmorphism effects Smooth animations Responsive design Accessible components Testing The frontend includes:\nComponent unit tests Integration tests for user flows E2E tests with Playwright Accessibility testing Next Steps With the frontend built, proceed to Deployment to deploy the complete application to AWS.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.9-deployment/","title":"Deployment","tags":[],"description":"","content":"Deploying InsightHR to Production This section covers deploying the complete InsightHR application to AWS, including frontend deployment to S3/CloudFront and backend Lambda functions.\nDeployment Overview The deployment process consists of:\nFrontend Deployment: Build and deploy React app to S3 CloudFront Configuration: Set up CDN for global distribution Lambda Deployment: Deploy all backend functions API Gateway Configuration: Set up REST API endpoints Environment Configuration: Configure environment variables DNS Setup (Optional): Configure custom domain Prerequisites Before deploying, ensure you have:\n✅ All DynamoDB tables created ✅ Cognito User Pool configured ✅ IAM roles and policies set up ✅ AWS CLI configured ✅ Node.js and npm installed ✅ Python 3.11+ installed Step 1: Frontend Deployment Build the React Application Navigate to the frontend directory and build the production bundle:\ncd insighthr-web # Install dependencies npm install # Build production bundle npm run build This creates an optimized production build in the dist/ directory.\nBuild Output:\ndist/ ├── index.html ├── assets/ │ ├── index-[hash].js │ ├── index-[hash].css │ └── [other assets] └── [other files] Create S3 Bucket Create an S3 bucket for hosting the static website:\n# Create bucket aws s3 mb s3://insighthr-web-app-sg --region ap-southeast-1 # Enable static website hosting aws s3 website s3://insighthr-web-app-sg \\ --index-document index.html \\ --error-document index.html Configure Bucket Policy Create a bucket policy to allow CloudFront access:\nbucket-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::insighthr-web-app-sg/*\u0026#34; } ] } Apply the policy:\naws s3api put-bucket-policy \\ --bucket insighthr-web-app-sg \\ --policy file://bucket-policy.json Upload Files to S3 # Sync dist folder to S3 aws s3 sync dist/ s3://insighthr-web-app-sg \\ --region ap-southeast-1 \\ --delete # Verify upload aws s3 ls s3://insighthr-web-app-sg --recursive Step 2: CloudFront Configuration Create CloudFront Distribution Using AWS Console:\nNavigate to CloudFront\nClick \u0026ldquo;Create Distribution\u0026rdquo;\nConfigure origin:\nOrigin domain: insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com Origin path: (leave empty) Name: S3-insighthr-web-app Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS Cache policy: CachingOptimized Settings:\nPrice class: Use all edge locations Alternate domain names (CNAMEs): insight-hr.io.vn, www.insight-hr.io.vn Custom SSL certificate: Request or import certificate Default root object: index.html Click \u0026ldquo;Create Distribution\u0026rdquo;\nUsing AWS CLI:\n# Create distribution configuration cat \u0026gt; cloudfront-config.json \u0026lt;\u0026lt; EOF { \u0026#34;CallerReference\u0026#34;: \u0026#34;insighthr-$(date +%s)\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;InsightHR CloudFront Distribution\u0026#34;, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;redirect-to-https\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 3, \u0026#34;Items\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;] }, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: false, \u0026#34;Cookies\u0026#34;: {\u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34;} }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000 }, \u0026#34;Enabled\u0026#34;: true } EOF # Create distribution aws cloudfront create-distribution \\ --distribution-config file://cloudfront-config.json Configure Custom Error Pages Set up error pages for SPA routing:\nGo to CloudFront distribution\nNavigate to \u0026ldquo;Error Pages\u0026rdquo; tab\nCreate custom error response:\nHTTP error code: 403 Customize error response: Yes Response page path: /index.html HTTP response code: 200 Repeat for error code 404\nStep 3: Lambda Deployment Package Lambda Functions For each Lambda function, create a deployment package:\nExample: Auth Login Handler\ncd lambda/auth # Create deployment package mkdir -p package pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy to Lambda aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::ACCOUNT_ID:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=ap-southeast-1_rzDtdAhvp, CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Deploy All Lambda Functions Create a deployment script:\ndeploy-all-lambdas.sh:\n#!/bin/bash FUNCTIONS=( \u0026#34;auth-login-handler\u0026#34; \u0026#34;auth-register-handler\u0026#34; \u0026#34;auth-google-handler\u0026#34; \u0026#34;employees-handler\u0026#34; \u0026#34;employees-bulk-handler\u0026#34; \u0026#34;performance-scores-handler\u0026#34; \u0026#34;chatbot-handler\u0026#34; \u0026#34;attendance-handler\u0026#34; ) for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deploying $func...\u0026#34; # Package and deploy logic here done Step 4: API Gateway Configuration Create REST API # Create API aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 Create Resources and Methods Example: Create /employees endpoint\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text) # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text) # Create /employees resource EMPLOYEES_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part employees \\ --query \u0026#39;id\u0026#39; \\ --output text) # Create GET method aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --authorization-type COGNITO_USER_POOLS \\ --authorizer-id $AUTHORIZER_ID # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:ACCOUNT_ID:function:insighthr-employees-handler/invocations Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; # Get API endpoint echo \u0026#34;API Endpoint: https://$API_ID.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 5: Environment Configuration Frontend Environment Variables Create .env.production file:\nVITE_API_BASE_URL=https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=ap-southeast-1_rzDtdAhvp VITE_COGNITO_CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5 Rebuild and redeploy frontend:\nnpm run build aws s3 sync dist/ s3://insighthr-web-app-sg --delete Lambda Environment Variables Update Lambda functions with environment variables:\naws lambda update-function-configuration \\ --function-name insighthr-employees-handler \\ --environment Variables=\u0026#34;{ AWS_REGION=ap-southeast-1, EMPLOYEES_TABLE=insighthr-employees-dev, USERS_TABLE=insighthr-users-dev }\u0026#34; Step 6: CloudFront Cache Invalidation After deploying frontend changes, invalidate CloudFront cache:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Create invalidation aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; Step 7: DNS Configuration (Optional) If using a custom domain:\nRequest SSL Certificate # Request certificate in us-east-1 (required for CloudFront) aws acm request-certificate \\ --domain-name insight-hr.io.vn \\ --subject-alternative-names www.insight-hr.io.vn \\ --validation-method DNS \\ --region us-east-1 Configure Route53 # Create hosted zone aws route53 create-hosted-zone \\ --name insight-hr.io.vn \\ --caller-reference $(date +%s) # Create A record for CloudFront cat \u0026gt; change-batch.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id ZONE_ID \\ --change-batch file://change-batch.json Deployment Checklist Before going live, verify:\nAll Lambda functions deployed and tested API Gateway endpoints configured DynamoDB tables populated with initial data Cognito User Pool configured Frontend built and deployed to S3 CloudFront distribution active SSL certificate validated (if using custom domain) DNS records configured (if using custom domain) Environment variables set correctly CORS configured on API Gateway CloudWatch logs enabled IAM roles and permissions verified Testing Deployment Test Frontend # Access CloudFront URL curl -I https://d2z6tht6rq32uy.cloudfront.net # Or custom domain curl -I https://insight-hr.io.vn Test API Endpoints # Test health check curl https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/health # Test authenticated endpoint (with token) curl -H \u0026#34;Authorization: Bearer YOUR_JWT_TOKEN\u0026#34; \\ https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/employees Continuous Deployment For automated deployments, consider:\nGitHub Actions: Automate build and deploy on push AWS CodePipeline: Full CI/CD pipeline AWS Amplify: Simplified frontend deployment Troubleshooting Frontend not loading:\nCheck S3 bucket policy Verify CloudFront distribution status Check browser console for errors API errors:\nVerify Lambda function logs in CloudWatch Check API Gateway configuration Verify Cognito authorizer setup CORS issues:\nConfigure CORS on API Gateway Check allowed origins match frontend domain Next Steps With deployment complete, proceed to Testing \u0026amp; Monitoring to set up CloudWatch monitoring and synthetic canaries.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.10-testing/","title":"Testing &amp; Monitoring","tags":[],"description":"","content":"Testing \u0026amp; Monitoring This section covers setting up comprehensive testing and monitoring for the InsightHR platform.\nOverview We\u0026rsquo;ll implement:\nCloudWatch Logs - Centralized logging CloudWatch Metrics - Performance monitoring CloudWatch Alarms - Automated alerting CloudWatch Synthetics - Automated testing X-Ray Tracing - Distributed tracing (optional) CloudWatch Logs All Lambda functions automatically log to CloudWatch Logs.\nView Logs:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Tail logs for a function aws logs tail /aws/lambda/insighthr-auth-login-handler \\ --follow \\ --region ap-southeast-1 Log Insights Queries:\nFind errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 20 Analyze response times:\nfields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) CloudWatch Metrics Monitor key metrics:\nLambda Metrics:\nInvocations Duration Errors Throttles Concurrent executions DynamoDB Metrics:\nRead/Write capacity units Throttled requests System errors API Gateway Metrics:\nRequest count Latency 4XX/5XX errors View Metrics:\n# Lambda invocations aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --start-time 2025-12-09T00:00:00Z \\ --end-time 2025-12-09T23:59:59Z \\ --period 3600 \\ --statistics Sum \\ --region ap-southeast-1 CloudWatch Alarms Create alarms for critical metrics:\nHigh Error Rate Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-error-rate \\ --alarm-description \u0026#34;Alert when Lambda error rate exceeds 5%\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --region ap-southeast-1 High Latency Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-latency \\ --alarm-description \u0026#34;Alert when API latency exceeds 2 seconds\u0026#34; \\ --metric-name Latency \\ --namespace AWS/ApiGateway \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 2000 \\ --comparison-operator GreaterThanThreshold \\ --region ap-southeast-1 CloudWatch Synthetics Canaries Automated testing with synthetic canaries:\n1. Login Canary - Tests login flow 2. Dashboard Canary - Tests dashboard loading 3. Chatbot Canary - Tests chatbot queries 4. Autoscoring Canary - Tests performance calculations\nCreate Login Canary:\n// login-canary.js const synthetics = require(\u0026#39;Synthetics\u0026#39;); const log = require(\u0026#39;SyntheticsLogger\u0026#39;); const loginFlow = async function () { const page = await synthetics.getPage(); // Navigate to login page await page.goto(\u0026#39;https://insight-hr.io.vn/login\u0026#39;, { waitUntil: \u0026#39;domcontentloaded\u0026#39;, timeout: 30000 }); // Fill login form await page.type(\u0026#39;#email\u0026#39;, \u0026#39;test@example.com\u0026#39;); await page.type(\u0026#39;#password\u0026#39;, \u0026#39;Test123!\u0026#39;); // Click login button await Promise.all([ page.waitForNavigation(), page.click(\u0026#39;button[type=\u0026#34;submit\u0026#34;]\u0026#39;) ]); // Verify successful login await page.waitForSelector(\u0026#39;.dashboard\u0026#39;, { timeout: 10000 }); log.info(\u0026#39;Login flow completed successfully\u0026#39;); }; exports.handler = async () =\u0026gt; { return await synthetics.executeStep(\u0026#39;LoginFlow\u0026#39;, loginFlow); }; Deploy Canary:\n# Create S3 bucket for canary artifacts aws s3 mb s3://insighthr-canary-artifacts --region ap-southeast-1 # Create canary aws synthetics create-canary \\ --name insighthr-login-canary \\ --artifact-s3-location s3://insighthr-canary-artifacts \\ --execution-role-arn arn:aws:iam::${ACCOUNT_ID}:role/CloudWatchSyntheticsRole \\ --schedule Expression=\u0026#34;rate(5 minutes)\u0026#34; \\ --runtime-version syn-nodejs-puppeteer-6.2 \\ --code file://login-canary.zip \\ --region ap-southeast-1 Dashboard Setup Create a CloudWatch Dashboard:\naws cloudwatch put-dashboard \\ --dashboard-name InsightHR-Dashboard \\ --dashboard-body file://dashboard-config.json \\ --region ap-southeast-1 dashboard-config.json:\n{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Metrics\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApiGateway\u0026#34;, \u0026#34;Count\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;4XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;5XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;API Gateway Metrics\u0026#34; } } ] } Testing Strategy Unit Tests:\nTest individual Lambda functions Mock DynamoDB and Cognito calls Verify business logic Integration Tests:\nTest API endpoints end-to-end Verify data flow through services Test authentication and authorization Load Tests:\nUse Artillery or k6 for load testing Test concurrent user scenarios Identify performance bottlenecks Example Load Test:\n# load-test.yml config: target: \u0026#39;https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#39; phases: - duration: 60 arrivalRate: 10 scenarios: - name: \u0026#34;Login and fetch employees\u0026#34; flow: - post: url: \u0026#34;/auth/login\u0026#34; json: email: \u0026#34;test@example.com\u0026#34; password: \u0026#34;Test123!\u0026#34; - get: url: \u0026#34;/employees\u0026#34; Best Practices ✅ Structured Logging: Use JSON format for logs ✅ Correlation IDs: Track requests across services ✅ Error Tracking: Log errors with context ✅ Performance Monitoring: Track response times ✅ Automated Testing: Run canaries regularly ✅ Alerting: Set up alarms for critical metrics ✅ Cost Monitoring: Track AWS costs\nTroubleshooting High Error Rates:\nCheck CloudWatch Logs for error details Verify IAM permissions Check DynamoDB capacity Review recent code changes High Latency:\nOptimize DynamoDB queries Add caching where appropriate Review Lambda memory allocation Check cold start times Failed Canaries:\nReview canary logs Check application availability Verify test credentials Update canary scripts if UI changed Monitoring Checklist CloudWatch Logs configured for all Lambda functions Key metrics tracked (invocations, errors, duration) Alarms set up for critical thresholds Synthetics canaries deployed and running Dashboard created for visualization Log retention policies configured Cost alerts configured On-call rotation established (for production) Next Steps With monitoring in place, proceed to Cleanup when you\u0026rsquo;re done with the workshop to avoid ongoing charges.\n"},{"uri":"https://ngocLong216.github.io/fcj_report/5-workshop/5.11-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Cleaning Up Resources To avoid ongoing charges, it\u0026rsquo;s important to delete all AWS resources created during this workshop. This section provides step-by-step instructions for cleaning up.\nImportant: Deleting resources is irreversible. Make sure you\u0026rsquo;ve backed up any data you want to keep before proceeding.\nCleanup Overview We\u0026rsquo;ll delete resources in the following order:\nCloudFront Distribution S3 Buckets API Gateway Lambda Functions DynamoDB Tables Cognito User Pool CloudWatch Logs IAM Roles and Policies Route53 (if configured) ACM Certificates (if created) Step 1: Delete CloudFront Distribution CloudFront distributions must be disabled before deletion.\nUsing AWS Console:\nNavigate to CloudFront Select the InsightHR distribution Click \u0026ldquo;Disable\u0026rdquo; Wait for status to change to \u0026ldquo;Disabled\u0026rdquo; (may take 15-20 minutes) Select the distribution again Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Get ETag ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) # Disable distribution aws cloudfront get-distribution-config --id $DIST_ID \u0026gt; dist-config.json # Edit dist-config.json: Set \u0026#34;Enabled\u0026#34;: false aws cloudfront update-distribution \\ --id $DIST_ID \\ --if-match $ETAG \\ --distribution-config file://dist-config.json # Wait for distribution to be disabled aws cloudfront wait distribution-deployed --id $DIST_ID # Delete distribution ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) aws cloudfront delete-distribution --id $DIST_ID --if-match $ETAG Step 2: Delete S3 Buckets Empty and delete the web app bucket:\n# Empty bucket aws s3 rm s3://insighthr-web-app-sg --recursive # Delete bucket aws s3 rb s3://insighthr-web-app-sg --force Delete canary artifacts bucket (if created):\naws s3 rm s3://insighthr-canary-artifacts --recursive aws s3 rb s3://insighthr-canary-artifacts --force Step 3: Delete API Gateway Using AWS Console:\nNavigate to API Gateway Select \u0026ldquo;InsightHR API\u0026rdquo; Click \u0026ldquo;Actions\u0026rdquo; → \u0026ldquo;Delete API\u0026rdquo; Confirm deletion Using AWS CLI:\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete API aws apigateway delete-rest-api \\ --rest-api-id $API_ID \\ --region ap-southeast-1 Step 4: Delete Lambda Functions Delete all Lambda functions:\n# List of functions to delete FUNCTIONS=( \u0026#34;insighthr-auth-login-handler\u0026#34; \u0026#34;insighthr-auth-register-handler\u0026#34; \u0026#34;insighthr-auth-google-handler\u0026#34; \u0026#34;insighthr-employees-handler\u0026#34; \u0026#34;insighthr-employees-bulk-handler\u0026#34; \u0026#34;insighthr-performance-scores-handler\u0026#34; \u0026#34;insighthr-chatbot-handler\u0026#34; \u0026#34;insighthr-attendance-handler\u0026#34; ) # Delete each function for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deleting $func...\u0026#34; aws lambda delete-function \\ --function-name $func \\ --region ap-southeast-1 done Step 5: Delete DynamoDB Tables Using AWS Console:\nNavigate to DynamoDB Select each table Click \u0026ldquo;Delete\u0026rdquo; Confirm by typing \u0026ldquo;delete\u0026rdquo; Using AWS CLI:\n# List of tables to delete TABLES=( \u0026#34;insighthr-users-dev\u0026#34; \u0026#34;insighthr-employees-dev\u0026#34; \u0026#34;insighthr-performance-scores-dev\u0026#34; \u0026#34;insighthr-attendance-history-dev\u0026#34; \u0026#34;insighthr-password-reset-requests-dev\u0026#34; \u0026#34;insighthr-kpis-dev\u0026#34; \u0026#34;insighthr-formulas-dev\u0026#34; \u0026#34;insighthr-data-tables-dev\u0026#34; \u0026#34;insighthr-notification-rules-dev\u0026#34; ) # Delete each table for table in \u0026#34;${TABLES[@]}\u0026#34;; do echo \u0026#34;Deleting $table...\u0026#34; aws dynamodb delete-table \\ --table-name $table \\ --region ap-southeast-1 done Step 6: Delete Cognito User Pool Using AWS Console:\nNavigate to Cognito Select \u0026ldquo;User Pools\u0026rdquo; Select the InsightHR user pool Click \u0026ldquo;Delete user pool\u0026rdquo; Type the pool name to confirm Using AWS CLI:\n# Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete user pool aws cognito-idp delete-user-pool \\ --user-pool-id $USER_POOL_ID \\ --region ap-southeast-1 Step 7: Delete CloudWatch Logs Delete log groups:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Delete each log group LOG_GROUPS=$(aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --query \u0026#39;logGroups[*].logGroupName\u0026#39; \\ --output text \\ --region ap-southeast-1) for log_group in $LOG_GROUPS; do echo \u0026#34;Deleting $log_group...\u0026#34; aws logs delete-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 done Delete CloudWatch Synthetics Canaries (if created):\n# List canaries aws synthetics describe-canaries \\ --region ap-southeast-1 # Delete each canary CANARIES=$(aws synthetics describe-canaries \\ --query \u0026#39;Canaries[?starts_with(Name, `insighthr`)].Name\u0026#39; \\ --output text \\ --region ap-southeast-1) for canary in $CANARIES; do echo \u0026#34;Deleting canary $canary...\u0026#34; aws synthetics delete-canary \\ --name $canary \\ --region ap-southeast-1 done Step 8: Delete IAM Roles and Policies Using AWS Console:\nNavigate to IAM Go to \u0026ldquo;Roles\u0026rdquo; Search for \u0026ldquo;insighthr\u0026rdquo; Select each role Detach policies Delete role Using AWS CLI:\n# List roles aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text # For each role, detach policies and delete ROLES=$(aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text) for role in $ROLES; do echo \u0026#34;Processing role $role...\u0026#34; # Detach managed policies POLICIES=$(aws iam list-attached-role-policies \\ --role-name $role \\ --query \u0026#39;AttachedPolicies[*].PolicyArn\u0026#39; \\ --output text) for policy in $POLICIES; do aws iam detach-role-policy \\ --role-name $role \\ --policy-arn $policy done # Delete inline policies INLINE_POLICIES=$(aws iam list-role-policies \\ --role-name $role \\ --query \u0026#39;PolicyNames[*]\u0026#39; \\ --output text) for policy in $INLINE_POLICIES; do aws iam delete-role-policy \\ --role-name $role \\ --policy-name $policy done # Delete role aws iam delete-role --role-name $role done Delete custom policies:\n# List custom policies aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text # Delete each policy POLICIES=$(aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text) for policy in $POLICIES; do echo \u0026#34;Deleting policy $policy...\u0026#34; # Delete all policy versions except default VERSIONS=$(aws iam list-policy-versions \\ --policy-arn $policy \\ --query \u0026#39;Versions[?!IsDefaultVersion].VersionId\u0026#39; \\ --output text) for version in $VERSIONS; do aws iam delete-policy-version \\ --policy-arn $policy \\ --version-id $version done # Delete policy aws iam delete-policy --policy-arn $policy done Step 9: Delete Route53 Resources (If Configured) Delete DNS records:\n# Get hosted zone ID ZONE_ID=$(aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;insight-hr.io.vn.\u0026#39;].Id\u0026#34; \\ --output text) # List and delete records (except NS and SOA) aws route53 list-resource-record-sets \\ --hosted-zone-id $ZONE_ID # Delete A records for CloudFront cat \u0026gt; delete-records.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id $ZONE_ID \\ --change-batch file://delete-records.json # Delete hosted zone aws route53 delete-hosted-zone --id $ZONE_ID Step 10: Delete ACM Certificates (If Created) Using AWS Console:\nNavigate to Certificate Manager (in us-east-1 region) Select the certificate Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# List certificates aws acm list-certificates \\ --region us-east-1 # Delete certificate CERT_ARN=$(aws acm list-certificates \\ --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;insight-hr.io.vn\u0026#39;].CertificateArn\u0026#34; \\ --output text \\ --region us-east-1) aws acm delete-certificate \\ --certificate-arn $CERT_ARN \\ --region us-east-1 Automated Cleanup Script Create a comprehensive cleanup script:\ncleanup-all.sh:\n#!/bin/bash set -e echo \u0026#34;Starting InsightHR cleanup...\u0026#34; # 1. Disable and delete CloudFront echo \u0026#34;Step 1: CloudFront...\u0026#34; # (Add CloudFront cleanup commands) # 2. Delete S3 buckets echo \u0026#34;Step 2: S3 Buckets...\u0026#34; aws s3 rm s3://insighthr-web-app-sg --recursive aws s3 rb s3://insighthr-web-app-sg --force # 3. Delete API Gateway echo \u0026#34;Step 3: API Gateway...\u0026#34; # (Add API Gateway cleanup commands) # 4. Delete Lambda functions echo \u0026#34;Step 4: Lambda Functions...\u0026#34; # (Add Lambda cleanup commands) # 5. Delete DynamoDB tables echo \u0026#34;Step 5: DynamoDB Tables...\u0026#34; # (Add DynamoDB cleanup commands) # 6. Delete Cognito echo \u0026#34;Step 6: Cognito User Pool...\u0026#34; # (Add Cognito cleanup commands) # 7. Delete CloudWatch logs echo \u0026#34;Step 7: CloudWatch Logs...\u0026#34; # (Add CloudWatch cleanup commands) # 8. Delete IAM roles echo \u0026#34;Step 8: IAM Roles and Policies...\u0026#34; # (Add IAM cleanup commands) echo \u0026#34;Cleanup complete!\u0026#34; Verification After cleanup, verify all resources are deleted:\n# Check S3 buckets aws s3 ls | grep insighthr # Check Lambda functions aws lambda list-functions \\ --query \u0026#39;Functions[?starts_with(FunctionName, `insighthr`)].FunctionName\u0026#39; \\ --region ap-southeast-1 # Check DynamoDB tables aws dynamodb list-tables \\ --query \u0026#39;TableNames[?starts_with(@, `insighthr`)]\u0026#39; \\ --region ap-southeast-1 # Check CloudFront distributions aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; # Check API Gateway aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --region ap-southeast-1 Cost Verification After cleanup, monitor your AWS billing:\nGo to AWS Billing Dashboard Check \u0026ldquo;Bills\u0026rdquo; for current month Verify no ongoing charges for deleted services Check \u0026ldquo;Cost Explorer\u0026rdquo; for trends Cleanup Checklist CloudFront distribution disabled and deleted S3 buckets emptied and deleted API Gateway deleted All Lambda functions deleted All DynamoDB tables deleted Cognito User Pool deleted CloudWatch log groups deleted CloudWatch Synthetics canaries deleted IAM roles and policies deleted Route53 hosted zone deleted (if created) ACM certificates deleted (if created) Billing dashboard checked for ongoing charges Troubleshooting Cleanup CloudFront won\u0026rsquo;t delete:\nEnsure distribution is fully disabled Wait 15-20 minutes after disabling Check for associated resources S3 bucket won\u0026rsquo;t delete:\nEnsure bucket is completely empty Check for versioned objects Disable versioning before deleting IAM role won\u0026rsquo;t delete:\nDetach all managed policies first Delete all inline policies Check for service-linked roles DynamoDB table deletion fails:\nWait for table to be in ACTIVE state Check for ongoing operations Verify IAM permissions Final Notes Best Practice: Always clean up resources after completing a workshop to avoid unexpected charges. Set up billing alerts to notify you of any ongoing costs.\nCongratulations! You\u0026rsquo;ve successfully completed the InsightHR workshop and cleaned up all resources. You\u0026rsquo;ve learned:\n✅ Serverless architecture design ✅ AWS Lambda and API Gateway ✅ DynamoDB data modeling ✅ Cognito authentication ✅ AWS Bedrock AI integration ✅ CloudFront CDN deployment ✅ Infrastructure as Code principles ✅ Cost optimization strategies Thank you for participating in this workshop!\n"},{"uri":"https://ngocLong216.github.io/fcj_report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ngocLong216.github.io/fcj_report/tags/","title":"Tags","tags":[],"description":"","content":""}]